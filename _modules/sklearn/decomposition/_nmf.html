

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>sklearn.decomposition._nmf &mdash; Cuvis AI 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../_static/documentation_options.js?v=01f34227"></script>
      <script src="../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            Cuvis AI
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../user/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user/quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user/tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user/concepts.html">Concepts</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../_autosummary/cuvis_ai.html">cuvis_ai</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Cuvis AI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">sklearn.decomposition._nmf</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for sklearn.decomposition._nmf</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Non-negative matrix factorization.&quot;&quot;&quot;</span>

<span class="c1"># Author: Vlad Niculae</span>
<span class="c1">#         Lars Buitinck</span>
<span class="c1">#         Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span>
<span class="c1">#         Tom Dupre la Tour</span>
<span class="c1"># License: BSD 3 clause</span>

<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sqrt</span>
<span class="kn">from</span> <span class="nn">numbers</span> <span class="kn">import</span> <span class="n">Integral</span><span class="p">,</span> <span class="n">Real</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.sparse</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">linalg</span>

<span class="kn">from</span> <span class="nn">.._config</span> <span class="kn">import</span> <span class="n">config_context</span>
<span class="kn">from</span> <span class="nn">..base</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">BaseEstimator</span><span class="p">,</span>
    <span class="n">ClassNamePrefixFeaturesOutMixin</span><span class="p">,</span>
    <span class="n">TransformerMixin</span><span class="p">,</span>
    <span class="n">_fit_context</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">..exceptions</span> <span class="kn">import</span> <span class="n">ConvergenceWarning</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">check_random_state</span><span class="p">,</span> <span class="n">gen_batches</span><span class="p">,</span> <span class="n">metadata_routing</span>
<span class="kn">from</span> <span class="nn">..utils._param_validation</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Hidden</span><span class="p">,</span>
    <span class="n">Interval</span><span class="p">,</span>
    <span class="n">StrOptions</span><span class="p">,</span>
    <span class="n">validate_params</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">..utils.deprecation</span> <span class="kn">import</span> <span class="n">_deprecate_Xt_in_inverse_transform</span>
<span class="kn">from</span> <span class="nn">..utils.extmath</span> <span class="kn">import</span> <span class="n">randomized_svd</span><span class="p">,</span> <span class="n">safe_sparse_dot</span><span class="p">,</span> <span class="n">squared_norm</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">check_is_fitted</span><span class="p">,</span>
    <span class="n">check_non_negative</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._cdnmf_fast</span> <span class="kn">import</span> <span class="n">_update_cdnmf_fast</span>

<span class="n">EPSILON</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>


<span class="k">def</span> <span class="nf">norm</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dot product-based Euclidean norm implementation.</span>

<span class="sd">    See: http://fa.bianp.net/blog/2011/computing-the-vector-norm/</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array-like</span>
<span class="sd">        Vector for which to compute the norm.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">squared_norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">trace_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Trace of np.dot(X, Y.T).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like</span>
<span class="sd">        First matrix.</span>
<span class="sd">    Y : array-like</span>
<span class="sd">        Second matrix.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>


<span class="k">def</span> <span class="nf">_check_init</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">whom</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;auto&quot;</span> <span class="ow">and</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Array with wrong first dimension passed to </span><span class="si">{</span><span class="n">whom</span><span class="si">}</span><span class="s2">. Expected </span><span class="si">{</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;auto&quot;</span> <span class="ow">and</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Array with wrong second dimension passed to </span><span class="si">{</span><span class="n">whom</span><span class="si">}</span><span class="s2">. Expected </span><span class="si">{</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
    <span class="n">check_non_negative</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">whom</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Array passed to </span><span class="si">{</span><span class="n">whom</span><span class="si">}</span><span class="s2"> is full of zeros.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_beta_divergence</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">square_root</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the beta-divergence of X and dot(W, H).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : float or array-like of shape (n_samples, n_features)</span>

<span class="sd">    W : float or array-like of shape (n_samples, n_components)</span>

<span class="sd">    H : float or array-like of shape (n_components, n_features)</span>

<span class="sd">    beta : float or {&#39;frobenius&#39;, &#39;kullback-leibler&#39;, &#39;itakura-saito&#39;}</span>
<span class="sd">        Parameter of the beta-divergence.</span>
<span class="sd">        If beta == 2, this is half the Frobenius *squared* norm.</span>
<span class="sd">        If beta == 1, this is the generalized Kullback-Leibler divergence.</span>
<span class="sd">        If beta == 0, this is the Itakura-Saito divergence.</span>
<span class="sd">        Else, this is the general beta-divergence.</span>

<span class="sd">    square_root : bool, default=False</span>
<span class="sd">        If True, return np.sqrt(2 * res)</span>
<span class="sd">        For beta == 2, it corresponds to the Frobenius norm.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">        res : float</span>
<span class="sd">            Beta divergence of X and np.dot(X, H).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">_beta_loss_to_float</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>

    <span class="c1"># The method can be called with scalars</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>

    <span class="c1"># Frobenius norm</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># Avoid the creation of the dense np.dot(W, H) if X is sparse.</span>
        <span class="k">if</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">norm_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
            <span class="n">norm_WH</span> <span class="o">=</span> <span class="n">trace_dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">multi_dot</span><span class="p">([</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">]),</span> <span class="n">H</span><span class="p">)</span>
            <span class="n">cross_prod</span> <span class="o">=</span> <span class="n">trace_dot</span><span class="p">((</span><span class="n">X</span> <span class="o">@</span> <span class="n">H</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">W</span><span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="p">(</span><span class="n">norm_X</span> <span class="o">+</span> <span class="n">norm_WH</span> <span class="o">-</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">cross_prod</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">squared_norm</span><span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">))</span> <span class="o">/</span> <span class="mf">2.0</span>

        <span class="k">if</span> <span class="n">square_root</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">res</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">res</span>

    <span class="k">if</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="c1"># compute np.dot(W, H) only where X is nonzero</span>
        <span class="n">WH_data</span> <span class="o">=</span> <span class="n">_special_sparse_dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">data</span>
        <span class="n">X_data</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">data</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">WH</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
        <span class="n">WH_data</span> <span class="o">=</span> <span class="n">WH</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">X_data</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="c1"># do not affect the zeros: here 0 ** (-1) = 0 and not infinity</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">X_data</span> <span class="o">&gt;</span> <span class="n">EPSILON</span>
    <span class="n">WH_data</span> <span class="o">=</span> <span class="n">WH_data</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
    <span class="n">X_data</span> <span class="o">=</span> <span class="n">X_data</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

    <span class="c1"># used to avoid division by zero</span>
    <span class="n">WH_data</span><span class="p">[</span><span class="n">WH_data</span> <span class="o">&lt;</span> <span class="n">EPSILON</span><span class="p">]</span> <span class="o">=</span> <span class="n">EPSILON</span>

    <span class="c1"># generalized Kullback-Leibler divergence</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># fast and memory efficient computation of np.sum(np.dot(W, H))</span>
        <span class="n">sum_WH</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="c1"># computes np.sum(X * log(X / WH)) only where X is nonzero</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">X_data</span> <span class="o">/</span> <span class="n">WH_data</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">div</span><span class="p">))</span>
        <span class="c1"># add full np.sum(np.dot(W, H)) - np.sum(X)</span>
        <span class="n">res</span> <span class="o">+=</span> <span class="n">sum_WH</span> <span class="o">-</span> <span class="n">X_data</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># Itakura-Saito divergence</span>
    <span class="k">elif</span> <span class="n">beta</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">X_data</span> <span class="o">/</span> <span class="n">WH_data</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">div</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">div</span><span class="p">))</span>

    <span class="c1"># beta-divergence, beta not in (0, 1, 2)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="c1"># slow loop, but memory efficient computation of :</span>
            <span class="c1"># np.sum(np.dot(W, H) ** beta)</span>
            <span class="n">sum_WH_beta</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">sum_WH_beta</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span> <span class="o">**</span> <span class="n">beta</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">sum_WH_beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">WH</span><span class="o">**</span><span class="n">beta</span><span class="p">)</span>

        <span class="n">sum_X_WH</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">WH_data</span> <span class="o">**</span> <span class="p">(</span><span class="n">beta</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_data</span><span class="o">**</span><span class="n">beta</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">sum_X_WH</span>
        <span class="n">res</span> <span class="o">+=</span> <span class="n">sum_WH_beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">beta</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">/=</span> <span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">beta</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">square_root</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># avoid negative number due to rounding errors</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">res</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">res</span>


<span class="k">def</span> <span class="nf">_special_sparse_dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes np.dot(W, H), only where X is non zero.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="n">ii</span><span class="p">,</span> <span class="n">jj</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
        <span class="n">n_vals</span> <span class="o">=</span> <span class="n">ii</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dot_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_vals</span><span class="p">)</span>
        <span class="n">n_components</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_vals</span> <span class="o">//</span> <span class="n">n_components</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_vals</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">start</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="n">dot_vals</span><span class="p">[</span><span class="n">batch</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">W</span><span class="p">[</span><span class="n">ii</span><span class="p">[</span><span class="n">batch</span><span class="p">],</span> <span class="p">:],</span> <span class="n">H</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="n">jj</span><span class="p">[</span><span class="n">batch</span><span class="p">],</span> <span class="p">:])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">)</span>

        <span class="n">WH</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">coo_matrix</span><span class="p">((</span><span class="n">dot_vals</span><span class="p">,</span> <span class="p">(</span><span class="n">ii</span><span class="p">,</span> <span class="n">jj</span><span class="p">)),</span> <span class="n">shape</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">WH</span><span class="o">.</span><span class="n">tocsr</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_beta_loss_to_float</span><span class="p">(</span><span class="n">beta_loss</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert string beta_loss to float.&quot;&quot;&quot;</span>
    <span class="n">beta_loss_map</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;frobenius&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;kullback-leibler&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;itakura-saito&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta_loss</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">beta_loss</span> <span class="o">=</span> <span class="n">beta_loss_map</span><span class="p">[</span><span class="n">beta_loss</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">beta_loss</span>


<span class="k">def</span> <span class="nf">_initialize_nmf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Algorithms for NMF initialization.</span>

<span class="sd">    Computes an initial guess for the non-negative</span>
<span class="sd">    rank k matrix approximation for X: X = WH.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like of shape (n_samples, n_features)</span>
<span class="sd">        The data matrix to be decomposed.</span>

<span class="sd">    n_components : int</span>
<span class="sd">        The number of components desired in the approximation.</span>

<span class="sd">    init :  {&#39;random&#39;, &#39;nndsvd&#39;, &#39;nndsvda&#39;, &#39;nndsvdar&#39;}, default=None</span>
<span class="sd">        Method used to initialize the procedure.</span>
<span class="sd">        Valid options:</span>

<span class="sd">        - None: &#39;nndsvda&#39; if n_components &lt;= min(n_samples, n_features),</span>
<span class="sd">            otherwise &#39;random&#39;.</span>

<span class="sd">        - &#39;random&#39;: non-negative random matrices, scaled with:</span>
<span class="sd">            sqrt(X.mean() / n_components)</span>

<span class="sd">        - &#39;nndsvd&#39;: Nonnegative Double Singular Value Decomposition (NNDSVD)</span>
<span class="sd">            initialization (better for sparseness)</span>

<span class="sd">        - &#39;nndsvda&#39;: NNDSVD with zeros filled with the average of X</span>
<span class="sd">            (better when sparsity is not desired)</span>

<span class="sd">        - &#39;nndsvdar&#39;: NNDSVD with zeros filled with small random values</span>
<span class="sd">            (generally faster, less accurate alternative to NNDSVDa</span>
<span class="sd">            for when sparsity is not desired)</span>

<span class="sd">        - &#39;custom&#39;: use custom matrices W and H</span>

<span class="sd">        .. versionchanged:: 1.1</span>
<span class="sd">            When `init=None` and n_components is less than n_samples and n_features</span>
<span class="sd">            defaults to `nndsvda` instead of `nndsvd`.</span>

<span class="sd">    eps : float, default=1e-6</span>
<span class="sd">        Truncate all values less then this in output to zero.</span>

<span class="sd">    random_state : int, RandomState instance or None, default=None</span>
<span class="sd">        Used when ``init`` == &#39;nndsvdar&#39; or &#39;random&#39;. Pass an int for</span>
<span class="sd">        reproducible results across multiple function calls.</span>
<span class="sd">        See :term:`Glossary &lt;random_state&gt;`.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    W : array-like of shape (n_samples, n_components)</span>
<span class="sd">        Initial guesses for solving X ~= WH.</span>

<span class="sd">    H : array-like of shape (n_components, n_features)</span>
<span class="sd">        Initial guesses for solving X ~= WH.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    C. Boutsidis, E. Gallopoulos: SVD based initialization: A head start for</span>
<span class="sd">    nonnegative matrix factorization - Pattern Recognition, 2008</span>
<span class="sd">    http://tinyurl.com/nndsvd</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_non_negative</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s2">&quot;NMF initialization&quot;</span><span class="p">)</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="n">init</span> <span class="o">!=</span> <span class="s2">&quot;random&quot;</span>
        <span class="ow">and</span> <span class="n">n_components</span> <span class="o">&gt;</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;init = &#39;</span><span class="si">{}</span><span class="s2">&#39; can only be used when &quot;</span>
            <span class="s2">&quot;n_components &lt;= min(n_samples, n_features)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">n_components</span> <span class="o">&lt;=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">):</span>
            <span class="n">init</span> <span class="o">=</span> <span class="s2">&quot;nndsvda&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">init</span> <span class="o">=</span> <span class="s2">&quot;random&quot;</span>

    <span class="c1"># Random initialization</span>
    <span class="k">if</span> <span class="n">init</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
        <span class="n">avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">/</span> <span class="n">n_components</span><span class="p">)</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">H</span> <span class="o">=</span> <span class="n">avg</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span>
            <span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">avg</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_components</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span>
            <span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">H</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">W</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span>

    <span class="c1"># NNDSVD initialization</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">randomized_svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>

    <span class="c1"># The leading singular triplet is non-negative</span>
    <span class="c1"># so it can be used as is for initialization.</span>
    <span class="n">W</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">U</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">H</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">V</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:])</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_components</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="n">j</span><span class="p">],</span> <span class="n">V</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># extract positive and negative parts of column vectors</span>
        <span class="n">x_p</span><span class="p">,</span> <span class="n">y_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">x_n</span><span class="p">,</span> <span class="n">y_n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

        <span class="c1"># and their norms</span>
        <span class="n">x_p_nrm</span><span class="p">,</span> <span class="n">y_p_nrm</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">x_p</span><span class="p">),</span> <span class="n">norm</span><span class="p">(</span><span class="n">y_p</span><span class="p">)</span>
        <span class="n">x_n_nrm</span><span class="p">,</span> <span class="n">y_n_nrm</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">x_n</span><span class="p">),</span> <span class="n">norm</span><span class="p">(</span><span class="n">y_n</span><span class="p">)</span>

        <span class="n">m_p</span><span class="p">,</span> <span class="n">m_n</span> <span class="o">=</span> <span class="n">x_p_nrm</span> <span class="o">*</span> <span class="n">y_p_nrm</span><span class="p">,</span> <span class="n">x_n_nrm</span> <span class="o">*</span> <span class="n">y_n_nrm</span>

        <span class="c1"># choose update</span>
        <span class="k">if</span> <span class="n">m_p</span> <span class="o">&gt;</span> <span class="n">m_n</span><span class="p">:</span>
            <span class="n">u</span> <span class="o">=</span> <span class="n">x_p</span> <span class="o">/</span> <span class="n">x_p_nrm</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">y_p</span> <span class="o">/</span> <span class="n">y_p_nrm</span>
            <span class="n">sigma</span> <span class="o">=</span> <span class="n">m_p</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">u</span> <span class="o">=</span> <span class="n">x_n</span> <span class="o">/</span> <span class="n">x_n_nrm</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">y_n</span> <span class="o">/</span> <span class="n">y_n_nrm</span>
            <span class="n">sigma</span> <span class="o">=</span> <span class="n">m_n</span>

        <span class="n">lbd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">S</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">sigma</span><span class="p">)</span>
        <span class="n">W</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">lbd</span> <span class="o">*</span> <span class="n">u</span>
        <span class="n">H</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">lbd</span> <span class="o">*</span> <span class="n">v</span>

    <span class="n">W</span><span class="p">[</span><span class="n">W</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">H</span><span class="p">[</span><span class="n">H</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">init</span> <span class="o">==</span> <span class="s2">&quot;nndsvd&quot;</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="k">elif</span> <span class="n">init</span> <span class="o">==</span> <span class="s2">&quot;nndsvda&quot;</span><span class="p">:</span>
        <span class="n">avg</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">W</span><span class="p">[</span><span class="n">W</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">avg</span>
        <span class="n">H</span><span class="p">[</span><span class="n">H</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">avg</span>
    <span class="k">elif</span> <span class="n">init</span> <span class="o">==</span> <span class="s2">&quot;nndsvdar&quot;</span><span class="p">:</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">avg</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">W</span><span class="p">[</span><span class="n">W</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">avg</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="p">[</span><span class="n">W</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]))</span> <span class="o">/</span> <span class="mi">100</span><span class="p">)</span>
        <span class="n">H</span><span class="p">[</span><span class="n">H</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">avg</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">H</span><span class="p">[</span><span class="n">H</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]))</span> <span class="o">/</span> <span class="mi">100</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid init parameter: got </span><span class="si">%r</span><span class="s2"> instead of one of </span><span class="si">%r</span><span class="s2">&quot;</span>
            <span class="o">%</span> <span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;random&quot;</span><span class="p">,</span> <span class="s2">&quot;nndsvd&quot;</span><span class="p">,</span> <span class="s2">&quot;nndsvda&quot;</span><span class="p">,</span> <span class="s2">&quot;nndsvdar&quot;</span><span class="p">))</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span>


<span class="k">def</span> <span class="nf">_update_coordinate_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">Ht</span><span class="p">,</span> <span class="n">l1_reg</span><span class="p">,</span> <span class="n">l2_reg</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">,</span> <span class="n">random_state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper function for _fit_coordinate_descent.</span>

<span class="sd">    Update W to minimize the objective function, iterating once over all</span>
<span class="sd">    coordinates. By symmetry, to update H, one can call</span>
<span class="sd">    _update_coordinate_descent(X.T, Ht, W, ...).</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_components</span> <span class="o">=</span> <span class="n">Ht</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">HHt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Ht</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Ht</span><span class="p">)</span>
    <span class="n">XHt</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Ht</span><span class="p">)</span>

    <span class="c1"># L2 regularization corresponds to increase of the diagonal of HHt</span>
    <span class="k">if</span> <span class="n">l2_reg</span> <span class="o">!=</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="c1"># adds l2_reg only on the diagonal</span>
        <span class="n">HHt</span><span class="o">.</span><span class="n">flat</span><span class="p">[::</span> <span class="n">n_components</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">l2_reg</span>
    <span class="c1"># L1 regularization corresponds to decrease of each element of XHt</span>
    <span class="k">if</span> <span class="n">l1_reg</span> <span class="o">!=</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">XHt</span> <span class="o">-=</span> <span class="n">l1_reg</span>

    <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
        <span class="n">permutation</span> <span class="o">=</span> <span class="n">random_state</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">n_components</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">permutation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_components</span><span class="p">)</span>
    <span class="c1"># The following seems to be required on 64-bit Windows w/ Python 3.5.</span>
    <span class="n">permutation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">permutation</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">intp</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_update_cdnmf_fast</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">HHt</span><span class="p">,</span> <span class="n">XHt</span><span class="p">,</span> <span class="n">permutation</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_fit_coordinate_descent</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">W</span><span class="p">,</span>
    <span class="n">H</span><span class="p">,</span>
    <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">l1_reg_W</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">l1_reg_H</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">l2_reg_W</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">l2_reg_H</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">update_H</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute Non-negative Matrix Factorization (NMF) with Coordinate Descent</span>

<span class="sd">    The objective function is minimized with an alternating minimization of W</span>
<span class="sd">    and H. Each minimization is done with a cyclic (up to a permutation of the</span>
<span class="sd">    features) Coordinate Descent.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like of shape (n_samples, n_features)</span>
<span class="sd">        Constant matrix.</span>

<span class="sd">    W : array-like of shape (n_samples, n_components)</span>
<span class="sd">        Initial guess for the solution.</span>

<span class="sd">    H : array-like of shape (n_components, n_features)</span>
<span class="sd">        Initial guess for the solution.</span>

<span class="sd">    tol : float, default=1e-4</span>
<span class="sd">        Tolerance of the stopping condition.</span>

<span class="sd">    max_iter : int, default=200</span>
<span class="sd">        Maximum number of iterations before timing out.</span>

<span class="sd">    l1_reg_W : float, default=0.</span>
<span class="sd">        L1 regularization parameter for W.</span>

<span class="sd">    l1_reg_H : float, default=0.</span>
<span class="sd">        L1 regularization parameter for H.</span>

<span class="sd">    l2_reg_W : float, default=0.</span>
<span class="sd">        L2 regularization parameter for W.</span>

<span class="sd">    l2_reg_H : float, default=0.</span>
<span class="sd">        L2 regularization parameter for H.</span>

<span class="sd">    update_H : bool, default=True</span>
<span class="sd">        Set to True, both W and H will be estimated from initial guesses.</span>
<span class="sd">        Set to False, only W will be estimated.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        The verbosity level.</span>

<span class="sd">    shuffle : bool, default=False</span>
<span class="sd">        If true, randomize the order of coordinates in the CD solver.</span>

<span class="sd">    random_state : int, RandomState instance or None, default=None</span>
<span class="sd">        Used to randomize the coordinates in the CD solver, when</span>
<span class="sd">        ``shuffle`` is set to ``True``. Pass an int for reproducible</span>
<span class="sd">        results across multiple function calls.</span>
<span class="sd">        See :term:`Glossary &lt;random_state&gt;`.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    W : ndarray of shape (n_samples, n_components)</span>
<span class="sd">        Solution to the non-negative least squares problem.</span>

<span class="sd">    H : ndarray of shape (n_components, n_features)</span>
<span class="sd">        Solution to the non-negative least squares problem.</span>

<span class="sd">    n_iter : int</span>
<span class="sd">        The number of iterations done by the algorithm.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] :doi:`&quot;Fast local algorithms for large scale nonnegative matrix and tensor</span>
<span class="sd">       factorizations&quot; &lt;10.1587/transfun.E92.A.708&gt;`</span>
<span class="sd">       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals</span>
<span class="sd">       of electronics, communications and computer sciences 92.3: 708-721, 2009.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># so W and Ht are both in C order in memory</span>
    <span class="n">Ht</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">H</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csr&quot;</span><span class="p">)</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">n_iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">violation</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="c1"># Update W</span>
        <span class="n">violation</span> <span class="o">+=</span> <span class="n">_update_coordinate_descent</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">Ht</span><span class="p">,</span> <span class="n">l1_reg_W</span><span class="p">,</span> <span class="n">l2_reg_W</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">,</span> <span class="n">rng</span>
        <span class="p">)</span>
        <span class="c1"># Update H</span>
        <span class="k">if</span> <span class="n">update_H</span><span class="p">:</span>
            <span class="n">violation</span> <span class="o">+=</span> <span class="n">_update_coordinate_descent</span><span class="p">(</span>
                <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Ht</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">l1_reg_H</span><span class="p">,</span> <span class="n">l2_reg_H</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">,</span> <span class="n">rng</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">n_iter</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">violation_init</span> <span class="o">=</span> <span class="n">violation</span>

        <span class="k">if</span> <span class="n">violation_init</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;violation:&quot;</span><span class="p">,</span> <span class="n">violation</span> <span class="o">/</span> <span class="n">violation_init</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">violation</span> <span class="o">/</span> <span class="n">violation_init</span> <span class="o">&lt;=</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Converged at iteration&quot;</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">Ht</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">n_iter</span>


<span class="k">def</span> <span class="nf">_multiplicative_update_w</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">W</span><span class="p">,</span>
    <span class="n">H</span><span class="p">,</span>
    <span class="n">beta_loss</span><span class="p">,</span>
    <span class="n">l1_reg_W</span><span class="p">,</span>
    <span class="n">l2_reg_W</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">,</span>
    <span class="n">H_sum</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">HHt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">XHt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">update_H</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Update W in Multiplicative Update NMF.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">beta_loss</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># Numerator</span>
        <span class="k">if</span> <span class="n">XHt</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">XHt</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">H</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">update_H</span><span class="p">:</span>
            <span class="c1"># avoid a copy of XHt, which will be re-computed (update_H=True)</span>
            <span class="n">numerator</span> <span class="o">=</span> <span class="n">XHt</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># preserve the XHt, which is not re-computed (update_H=False)</span>
            <span class="n">numerator</span> <span class="o">=</span> <span class="n">XHt</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># Denominator</span>
        <span class="k">if</span> <span class="n">HHt</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">HHt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">H</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">HHt</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Numerator</span>
        <span class="c1"># if X is sparse, compute WH only where X is non zero</span>
        <span class="n">WH_safe_X</span> <span class="o">=</span> <span class="n">_special_sparse_dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">WH_safe_X_data</span> <span class="o">=</span> <span class="n">WH_safe_X</span><span class="o">.</span><span class="n">data</span>
            <span class="n">X_data</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">data</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">WH_safe_X_data</span> <span class="o">=</span> <span class="n">WH_safe_X</span>
            <span class="n">X_data</span> <span class="o">=</span> <span class="n">X</span>
            <span class="c1"># copy used in the Denominator</span>
            <span class="n">WH</span> <span class="o">=</span> <span class="n">WH_safe_X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">beta_loss</span> <span class="o">-</span> <span class="mf">1.0</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">WH</span><span class="p">[</span><span class="n">WH</span> <span class="o">&lt;</span> <span class="n">EPSILON</span><span class="p">]</span> <span class="o">=</span> <span class="n">EPSILON</span>

        <span class="c1"># to avoid taking a negative power of zero</span>
        <span class="k">if</span> <span class="n">beta_loss</span> <span class="o">-</span> <span class="mf">2.0</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">WH_safe_X_data</span><span class="p">[</span><span class="n">WH_safe_X_data</span> <span class="o">&lt;</span> <span class="n">EPSILON</span><span class="p">]</span> <span class="o">=</span> <span class="n">EPSILON</span>

        <span class="k">if</span> <span class="n">beta_loss</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">WH_safe_X_data</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">WH_safe_X_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">beta_loss</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># speeds up computation time</span>
            <span class="c1"># refer to /numpy/numpy/issues/9363</span>
            <span class="n">WH_safe_X_data</span> <span class="o">**=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">WH_safe_X_data</span> <span class="o">**=</span> <span class="mi">2</span>
            <span class="c1"># element-wise multiplication</span>
            <span class="n">WH_safe_X_data</span> <span class="o">*=</span> <span class="n">X_data</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">WH_safe_X_data</span> <span class="o">**=</span> <span class="n">beta_loss</span> <span class="o">-</span> <span class="mi">2</span>
            <span class="c1"># element-wise multiplication</span>
            <span class="n">WH_safe_X_data</span> <span class="o">*=</span> <span class="n">X_data</span>

        <span class="c1"># here numerator = dot(X * (dot(W, H) ** (beta_loss - 2)), H.T)</span>
        <span class="n">numerator</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">WH_safe_X</span><span class="p">,</span> <span class="n">H</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

        <span class="c1"># Denominator</span>
        <span class="k">if</span> <span class="n">beta_loss</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">H_sum</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">H_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># shape(n_components, )</span>
            <span class="n">denominator</span> <span class="o">=</span> <span class="n">H_sum</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># computation of WHHt = dot(dot(W, H) ** beta_loss - 1, H.T)</span>
            <span class="k">if</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                <span class="c1"># memory efficient computation</span>
                <span class="c1"># (compute row by row, avoiding the dense matrix WH)</span>
                <span class="n">WHHt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                    <span class="n">WHi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">H</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">beta_loss</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">WHi</span><span class="p">[</span><span class="n">WHi</span> <span class="o">&lt;</span> <span class="n">EPSILON</span><span class="p">]</span> <span class="o">=</span> <span class="n">EPSILON</span>
                    <span class="n">WHi</span> <span class="o">**=</span> <span class="n">beta_loss</span> <span class="o">-</span> <span class="mi">1</span>
                    <span class="n">WHHt</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">WHi</span><span class="p">,</span> <span class="n">H</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">WH</span> <span class="o">**=</span> <span class="n">beta_loss</span> <span class="o">-</span> <span class="mi">1</span>
                <span class="n">WHHt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">WH</span><span class="p">,</span> <span class="n">H</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
            <span class="n">denominator</span> <span class="o">=</span> <span class="n">WHHt</span>

    <span class="c1"># Add L1 and L2 regularization</span>
    <span class="k">if</span> <span class="n">l1_reg_W</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">denominator</span> <span class="o">+=</span> <span class="n">l1_reg_W</span>
    <span class="k">if</span> <span class="n">l2_reg_W</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">denominator</span> <span class="o">+</span> <span class="n">l2_reg_W</span> <span class="o">*</span> <span class="n">W</span>
    <span class="n">denominator</span><span class="p">[</span><span class="n">denominator</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">EPSILON</span>

    <span class="n">numerator</span> <span class="o">/=</span> <span class="n">denominator</span>
    <span class="n">delta_W</span> <span class="o">=</span> <span class="n">numerator</span>

    <span class="c1"># gamma is in ]0, 1]</span>
    <span class="k">if</span> <span class="n">gamma</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">delta_W</span> <span class="o">**=</span> <span class="n">gamma</span>

    <span class="n">W</span> <span class="o">*=</span> <span class="n">delta_W</span>

    <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">H_sum</span><span class="p">,</span> <span class="n">HHt</span><span class="p">,</span> <span class="n">XHt</span>


<span class="k">def</span> <span class="nf">_multiplicative_update_h</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">beta_loss</span><span class="p">,</span> <span class="n">l1_reg_H</span><span class="p">,</span> <span class="n">l2_reg_H</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">A</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="kc">None</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;update H in Multiplicative Update NMF.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">beta_loss</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">numerator</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">multi_dot</span><span class="p">([</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">])</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Numerator</span>
        <span class="n">WH_safe_X</span> <span class="o">=</span> <span class="n">_special_sparse_dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">WH_safe_X_data</span> <span class="o">=</span> <span class="n">WH_safe_X</span><span class="o">.</span><span class="n">data</span>
            <span class="n">X_data</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">data</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">WH_safe_X_data</span> <span class="o">=</span> <span class="n">WH_safe_X</span>
            <span class="n">X_data</span> <span class="o">=</span> <span class="n">X</span>
            <span class="c1"># copy used in the Denominator</span>
            <span class="n">WH</span> <span class="o">=</span> <span class="n">WH_safe_X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">beta_loss</span> <span class="o">-</span> <span class="mf">1.0</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">WH</span><span class="p">[</span><span class="n">WH</span> <span class="o">&lt;</span> <span class="n">EPSILON</span><span class="p">]</span> <span class="o">=</span> <span class="n">EPSILON</span>

        <span class="c1"># to avoid division by zero</span>
        <span class="k">if</span> <span class="n">beta_loss</span> <span class="o">-</span> <span class="mf">2.0</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">WH_safe_X_data</span><span class="p">[</span><span class="n">WH_safe_X_data</span> <span class="o">&lt;</span> <span class="n">EPSILON</span><span class="p">]</span> <span class="o">=</span> <span class="n">EPSILON</span>

        <span class="k">if</span> <span class="n">beta_loss</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">WH_safe_X_data</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">WH_safe_X_data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">beta_loss</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># speeds up computation time</span>
            <span class="c1"># refer to /numpy/numpy/issues/9363</span>
            <span class="n">WH_safe_X_data</span> <span class="o">**=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">WH_safe_X_data</span> <span class="o">**=</span> <span class="mi">2</span>
            <span class="c1"># element-wise multiplication</span>
            <span class="n">WH_safe_X_data</span> <span class="o">*=</span> <span class="n">X_data</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">WH_safe_X_data</span> <span class="o">**=</span> <span class="n">beta_loss</span> <span class="o">-</span> <span class="mi">2</span>
            <span class="c1"># element-wise multiplication</span>
            <span class="n">WH_safe_X_data</span> <span class="o">*=</span> <span class="n">X_data</span>

        <span class="c1"># here numerator = dot(W.T, (dot(W, H) ** (beta_loss - 2)) * X)</span>
        <span class="n">numerator</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">WH_safe_X</span><span class="p">)</span>

        <span class="c1"># Denominator</span>
        <span class="k">if</span> <span class="n">beta_loss</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">W_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># shape(n_components, )</span>
            <span class="n">W_sum</span><span class="p">[</span><span class="n">W_sum</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
            <span class="n">denominator</span> <span class="o">=</span> <span class="n">W_sum</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

        <span class="c1"># beta_loss not in (1, 2)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># computation of WtWH = dot(W.T, dot(W, H) ** beta_loss - 1)</span>
            <span class="k">if</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                <span class="c1"># memory efficient computation</span>
                <span class="c1"># (compute column by column, avoiding the dense matrix WH)</span>
                <span class="n">WtWH</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">H</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="n">WHi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span>
                    <span class="k">if</span> <span class="n">beta_loss</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">WHi</span><span class="p">[</span><span class="n">WHi</span> <span class="o">&lt;</span> <span class="n">EPSILON</span><span class="p">]</span> <span class="o">=</span> <span class="n">EPSILON</span>
                    <span class="n">WHi</span> <span class="o">**=</span> <span class="n">beta_loss</span> <span class="o">-</span> <span class="mi">1</span>
                    <span class="n">WtWH</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">WHi</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">WH</span> <span class="o">**=</span> <span class="n">beta_loss</span> <span class="o">-</span> <span class="mi">1</span>
                <span class="n">WtWH</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">WH</span><span class="p">)</span>
            <span class="n">denominator</span> <span class="o">=</span> <span class="n">WtWH</span>

    <span class="c1"># Add L1 and L2 regularization</span>
    <span class="k">if</span> <span class="n">l1_reg_H</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">denominator</span> <span class="o">+=</span> <span class="n">l1_reg_H</span>
    <span class="k">if</span> <span class="n">l2_reg_H</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">denominator</span> <span class="o">+</span> <span class="n">l2_reg_H</span> <span class="o">*</span> <span class="n">H</span>
    <span class="n">denominator</span><span class="p">[</span><span class="n">denominator</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">EPSILON</span>

    <span class="k">if</span> <span class="n">A</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">B</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Updates for the online nmf</span>
        <span class="k">if</span> <span class="n">gamma</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">H</span> <span class="o">**=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">gamma</span>
        <span class="n">numerator</span> <span class="o">*=</span> <span class="n">H</span>
        <span class="n">A</span> <span class="o">*=</span> <span class="n">rho</span>
        <span class="n">B</span> <span class="o">*=</span> <span class="n">rho</span>
        <span class="n">A</span> <span class="o">+=</span> <span class="n">numerator</span>
        <span class="n">B</span> <span class="o">+=</span> <span class="n">denominator</span>
        <span class="n">H</span> <span class="o">=</span> <span class="n">A</span> <span class="o">/</span> <span class="n">B</span>

        <span class="k">if</span> <span class="n">gamma</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">H</span> <span class="o">**=</span> <span class="n">gamma</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">delta_H</span> <span class="o">=</span> <span class="n">numerator</span>
        <span class="n">delta_H</span> <span class="o">/=</span> <span class="n">denominator</span>
        <span class="k">if</span> <span class="n">gamma</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">delta_H</span> <span class="o">**=</span> <span class="n">gamma</span>
        <span class="n">H</span> <span class="o">*=</span> <span class="n">delta_H</span>

    <span class="k">return</span> <span class="n">H</span>


<span class="k">def</span> <span class="nf">_fit_multiplicative_update</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">W</span><span class="p">,</span>
    <span class="n">H</span><span class="p">,</span>
    <span class="n">beta_loss</span><span class="o">=</span><span class="s2">&quot;frobenius&quot;</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
    <span class="n">l1_reg_W</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">l1_reg_H</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">l2_reg_W</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">l2_reg_H</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">update_H</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute Non-negative Matrix Factorization with Multiplicative Update.</span>

<span class="sd">    The objective function is _beta_divergence(X, WH) and is minimized with an</span>
<span class="sd">    alternating minimization of W and H. Each minimization is done with a</span>
<span class="sd">    Multiplicative Update.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like of shape (n_samples, n_features)</span>
<span class="sd">        Constant input matrix.</span>

<span class="sd">    W : array-like of shape (n_samples, n_components)</span>
<span class="sd">        Initial guess for the solution.</span>

<span class="sd">    H : array-like of shape (n_components, n_features)</span>
<span class="sd">        Initial guess for the solution.</span>

<span class="sd">    beta_loss : float or {&#39;frobenius&#39;, &#39;kullback-leibler&#39;, \</span>
<span class="sd">            &#39;itakura-saito&#39;}, default=&#39;frobenius&#39;</span>
<span class="sd">        String must be in {&#39;frobenius&#39;, &#39;kullback-leibler&#39;, &#39;itakura-saito&#39;}.</span>
<span class="sd">        Beta divergence to be minimized, measuring the distance between X</span>
<span class="sd">        and the dot product WH. Note that values different from &#39;frobenius&#39;</span>
<span class="sd">        (or 2) and &#39;kullback-leibler&#39; (or 1) lead to significantly slower</span>
<span class="sd">        fits. Note that for beta_loss &lt;= 0 (or &#39;itakura-saito&#39;), the input</span>
<span class="sd">        matrix X cannot contain zeros.</span>

<span class="sd">    max_iter : int, default=200</span>
<span class="sd">        Number of iterations.</span>

<span class="sd">    tol : float, default=1e-4</span>
<span class="sd">        Tolerance of the stopping condition.</span>

<span class="sd">    l1_reg_W : float, default=0.</span>
<span class="sd">        L1 regularization parameter for W.</span>

<span class="sd">    l1_reg_H : float, default=0.</span>
<span class="sd">        L1 regularization parameter for H.</span>

<span class="sd">    l2_reg_W : float, default=0.</span>
<span class="sd">        L2 regularization parameter for W.</span>

<span class="sd">    l2_reg_H : float, default=0.</span>
<span class="sd">        L2 regularization parameter for H.</span>

<span class="sd">    update_H : bool, default=True</span>
<span class="sd">        Set to True, both W and H will be estimated from initial guesses.</span>
<span class="sd">        Set to False, only W will be estimated.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        The verbosity level.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    W : ndarray of shape (n_samples, n_components)</span>
<span class="sd">        Solution to the non-negative least squares problem.</span>

<span class="sd">    H : ndarray of shape (n_components, n_features)</span>
<span class="sd">        Solution to the non-negative least squares problem.</span>

<span class="sd">    n_iter : int</span>
<span class="sd">        The number of iterations done by the algorithm.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Lee, D. D., &amp; Seung, H., S. (2001). Algorithms for Non-negative Matrix</span>
<span class="sd">    Factorization. Adv. Neural Inform. Process. Syst.. 13.</span>
<span class="sd">    Fevotte, C., &amp; Idier, J. (2011). Algorithms for nonnegative matrix</span>
<span class="sd">    factorization with the beta-divergence. Neural Computation, 23(9).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="n">beta_loss</span> <span class="o">=</span> <span class="n">_beta_loss_to_float</span><span class="p">(</span><span class="n">beta_loss</span><span class="p">)</span>

    <span class="c1"># gamma for Maximization-Minimization (MM) algorithm [Fevotte 2011]</span>
    <span class="k">if</span> <span class="n">beta_loss</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">-</span> <span class="n">beta_loss</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">beta_loss</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">beta_loss</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="c1"># used for the convergence criterion</span>
    <span class="n">error_at_init</span> <span class="o">=</span> <span class="n">_beta_divergence</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">beta_loss</span><span class="p">,</span> <span class="n">square_root</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">previous_error</span> <span class="o">=</span> <span class="n">error_at_init</span>

    <span class="n">H_sum</span><span class="p">,</span> <span class="n">HHt</span><span class="p">,</span> <span class="n">XHt</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">n_iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># update W</span>
        <span class="c1"># H_sum, HHt and XHt are saved and reused if not update_H</span>
        <span class="n">W</span><span class="p">,</span> <span class="n">H_sum</span><span class="p">,</span> <span class="n">HHt</span><span class="p">,</span> <span class="n">XHt</span> <span class="o">=</span> <span class="n">_multiplicative_update_w</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span>
            <span class="n">W</span><span class="p">,</span>
            <span class="n">H</span><span class="p">,</span>
            <span class="n">beta_loss</span><span class="o">=</span><span class="n">beta_loss</span><span class="p">,</span>
            <span class="n">l1_reg_W</span><span class="o">=</span><span class="n">l1_reg_W</span><span class="p">,</span>
            <span class="n">l2_reg_W</span><span class="o">=</span><span class="n">l2_reg_W</span><span class="p">,</span>
            <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">H_sum</span><span class="o">=</span><span class="n">H_sum</span><span class="p">,</span>
            <span class="n">HHt</span><span class="o">=</span><span class="n">HHt</span><span class="p">,</span>
            <span class="n">XHt</span><span class="o">=</span><span class="n">XHt</span><span class="p">,</span>
            <span class="n">update_H</span><span class="o">=</span><span class="n">update_H</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># necessary for stability with beta_loss &lt; 1</span>
        <span class="k">if</span> <span class="n">beta_loss</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">W</span><span class="p">[</span><span class="n">W</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="c1"># update H (only at fit or fit_transform)</span>
        <span class="k">if</span> <span class="n">update_H</span><span class="p">:</span>
            <span class="n">H</span> <span class="o">=</span> <span class="n">_multiplicative_update_h</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span>
                <span class="n">W</span><span class="p">,</span>
                <span class="n">H</span><span class="p">,</span>
                <span class="n">beta_loss</span><span class="o">=</span><span class="n">beta_loss</span><span class="p">,</span>
                <span class="n">l1_reg_H</span><span class="o">=</span><span class="n">l1_reg_H</span><span class="p">,</span>
                <span class="n">l2_reg_H</span><span class="o">=</span><span class="n">l2_reg_H</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># These values will be recomputed since H changed</span>
            <span class="n">H_sum</span><span class="p">,</span> <span class="n">HHt</span><span class="p">,</span> <span class="n">XHt</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

            <span class="c1"># necessary for stability with beta_loss &lt; 1</span>
            <span class="k">if</span> <span class="n">beta_loss</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">H</span><span class="p">[</span><span class="n">H</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="c1"># test convergence criterion every 10 iterations</span>
        <span class="k">if</span> <span class="n">tol</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n_iter</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">_beta_divergence</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">beta_loss</span><span class="p">,</span> <span class="n">square_root</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="n">iter_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">&quot;Epoch </span><span class="si">%02d</span><span class="s2"> reached after </span><span class="si">%.3f</span><span class="s2"> seconds, error: </span><span class="si">%f</span><span class="s2">&quot;</span>
                    <span class="o">%</span> <span class="p">(</span><span class="n">n_iter</span><span class="p">,</span> <span class="n">iter_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span><span class="n">previous_error</span> <span class="o">-</span> <span class="n">error</span><span class="p">)</span> <span class="o">/</span> <span class="n">error_at_init</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">previous_error</span> <span class="o">=</span> <span class="n">error</span>

    <span class="c1"># do not print if we have already printed in the convergence test</span>
    <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="p">(</span><span class="n">tol</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">n_iter</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="s2">&quot;Epoch </span><span class="si">%02d</span><span class="s2"> reached after </span><span class="si">%.3f</span><span class="s2"> seconds.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_iter</span><span class="p">,</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">n_iter</span>


<span class="nd">@validate_params</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;X&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;array-like&quot;</span><span class="p">,</span> <span class="s2">&quot;sparse matrix&quot;</span><span class="p">],</span>
        <span class="s2">&quot;W&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;array-like&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
        <span class="s2">&quot;H&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;array-like&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
        <span class="s2">&quot;update_H&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;boolean&quot;</span><span class="p">],</span>
    <span class="p">},</span>
    <span class="n">prefer_skip_nested_validation</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">non_negative_factorization</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">W</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">H</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_components</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">update_H</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;cd&quot;</span><span class="p">,</span>
    <span class="n">beta_loss</span><span class="o">=</span><span class="s2">&quot;frobenius&quot;</span><span class="p">,</span>
    <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">alpha_W</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">alpha_H</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
    <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute Non-negative Matrix Factorization (NMF).</span>

<span class="sd">    Find two non-negative matrices (W, H) whose product approximates the non-</span>
<span class="sd">    negative matrix X. This factorization can be used for example for</span>
<span class="sd">    dimensionality reduction, source separation or topic extraction.</span>

<span class="sd">    The objective function is:</span>

<span class="sd">        .. math::</span>

<span class="sd">            L(W, H) &amp;= 0.5 * ||X - WH||_{loss}^2</span>

<span class="sd">            &amp;+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1</span>

<span class="sd">            &amp;+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1</span>

<span class="sd">            &amp;+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2</span>

<span class="sd">            &amp;+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2</span>

<span class="sd">    Where:</span>

<span class="sd">    :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)</span>

<span class="sd">    :math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)</span>

<span class="sd">    The generic norm :math:`||X - WH||_{loss}^2` may represent</span>
<span class="sd">    the Frobenius norm or another supported beta-divergence loss.</span>
<span class="sd">    The choice between options is controlled by the `beta_loss` parameter.</span>

<span class="sd">    The regularization terms are scaled by `n_features` for `W` and by `n_samples` for</span>
<span class="sd">    `H` to keep their impact balanced with respect to one another and to the data fit</span>
<span class="sd">    term as independent as possible of the size `n_samples` of the training set.</span>

<span class="sd">    The objective function is minimized with an alternating minimization of W</span>
<span class="sd">    and H. If H is given and update_H=False, it solves for W only.</span>

<span class="sd">    Note that the transformed data is named W and the components matrix is named H. In</span>
<span class="sd">    the NMF literature, the naming convention is usually the opposite since the data</span>
<span class="sd">    matrix X is transposed.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Constant matrix.</span>

<span class="sd">    W : array-like of shape (n_samples, n_components), default=None</span>
<span class="sd">        If `init=&#39;custom&#39;`, it is used as initial guess for the solution.</span>
<span class="sd">        If `update_H=False`, it is initialised as an array of zeros, unless</span>
<span class="sd">        `solver=&#39;mu&#39;`, then it is filled with values calculated by</span>
<span class="sd">        `np.sqrt(X.mean() / self._n_components)`.</span>
<span class="sd">        If `None`, uses the initialisation method specified in `init`.</span>

<span class="sd">    H : array-like of shape (n_components, n_features), default=None</span>
<span class="sd">        If `init=&#39;custom&#39;`, it is used as initial guess for the solution.</span>
<span class="sd">        If `update_H=False`, it is used as a constant, to solve for W only.</span>
<span class="sd">        If `None`, uses the initialisation method specified in `init`.</span>

<span class="sd">    n_components : int or {&#39;auto&#39;} or None, default=None</span>
<span class="sd">        Number of components, if n_components is not set all features</span>
<span class="sd">        are kept.</span>
<span class="sd">        If `n_components=&#39;auto&#39;`, the number of components is automatically inferred</span>
<span class="sd">        from `W` or `H` shapes.</span>

<span class="sd">        .. versionchanged:: 1.4</span>
<span class="sd">            Added `&#39;auto&#39;` value.</span>

<span class="sd">    init : {&#39;random&#39;, &#39;nndsvd&#39;, &#39;nndsvda&#39;, &#39;nndsvdar&#39;, &#39;custom&#39;}, default=None</span>
<span class="sd">        Method used to initialize the procedure.</span>

<span class="sd">        Valid options:</span>

<span class="sd">        - None: &#39;nndsvda&#39; if n_components &lt; n_features, otherwise &#39;random&#39;.</span>
<span class="sd">        - &#39;random&#39;: non-negative random matrices, scaled with:</span>
<span class="sd">          `sqrt(X.mean() / n_components)`</span>
<span class="sd">        - &#39;nndsvd&#39;: Nonnegative Double Singular Value Decomposition (NNDSVD)</span>
<span class="sd">          initialization (better for sparseness)</span>
<span class="sd">        - &#39;nndsvda&#39;: NNDSVD with zeros filled with the average of X</span>
<span class="sd">          (better when sparsity is not desired)</span>
<span class="sd">        - &#39;nndsvdar&#39;: NNDSVD with zeros filled with small random values</span>
<span class="sd">          (generally faster, less accurate alternative to NNDSVDa</span>
<span class="sd">          for when sparsity is not desired)</span>
<span class="sd">        - &#39;custom&#39;: If `update_H=True`, use custom matrices W and H which must both</span>
<span class="sd">          be provided. If `update_H=False`, then only custom matrix H is used.</span>

<span class="sd">        .. versionchanged:: 0.23</span>
<span class="sd">            The default value of `init` changed from &#39;random&#39; to None in 0.23.</span>

<span class="sd">        .. versionchanged:: 1.1</span>
<span class="sd">            When `init=None` and n_components is less than n_samples and n_features</span>
<span class="sd">            defaults to `nndsvda` instead of `nndsvd`.</span>

<span class="sd">    update_H : bool, default=True</span>
<span class="sd">        Set to True, both W and H will be estimated from initial guesses.</span>
<span class="sd">        Set to False, only W will be estimated.</span>

<span class="sd">    solver : {&#39;cd&#39;, &#39;mu&#39;}, default=&#39;cd&#39;</span>
<span class="sd">        Numerical solver to use:</span>

<span class="sd">        - &#39;cd&#39; is a Coordinate Descent solver that uses Fast Hierarchical</span>
<span class="sd">          Alternating Least Squares (Fast HALS).</span>
<span class="sd">        - &#39;mu&#39; is a Multiplicative Update solver.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           Coordinate Descent solver.</span>

<span class="sd">        .. versionadded:: 0.19</span>
<span class="sd">           Multiplicative Update solver.</span>

<span class="sd">    beta_loss : float or {&#39;frobenius&#39;, &#39;kullback-leibler&#39;, \</span>
<span class="sd">            &#39;itakura-saito&#39;}, default=&#39;frobenius&#39;</span>
<span class="sd">        Beta divergence to be minimized, measuring the distance between X</span>
<span class="sd">        and the dot product WH. Note that values different from &#39;frobenius&#39;</span>
<span class="sd">        (or 2) and &#39;kullback-leibler&#39; (or 1) lead to significantly slower</span>
<span class="sd">        fits. Note that for beta_loss &lt;= 0 (or &#39;itakura-saito&#39;), the input</span>
<span class="sd">        matrix X cannot contain zeros. Used only in &#39;mu&#39; solver.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    tol : float, default=1e-4</span>
<span class="sd">        Tolerance of the stopping condition.</span>

<span class="sd">    max_iter : int, default=200</span>
<span class="sd">        Maximum number of iterations before timing out.</span>

<span class="sd">    alpha_W : float, default=0.0</span>
<span class="sd">        Constant that multiplies the regularization terms of `W`. Set it to zero</span>
<span class="sd">        (default) to have no regularization on `W`.</span>

<span class="sd">        .. versionadded:: 1.0</span>

<span class="sd">    alpha_H : float or &quot;same&quot;, default=&quot;same&quot;</span>
<span class="sd">        Constant that multiplies the regularization terms of `H`. Set it to zero to</span>
<span class="sd">        have no regularization on `H`. If &quot;same&quot; (default), it takes the same value as</span>
<span class="sd">        `alpha_W`.</span>

<span class="sd">        .. versionadded:: 1.0</span>

<span class="sd">    l1_ratio : float, default=0.0</span>
<span class="sd">        The regularization mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.</span>
<span class="sd">        For l1_ratio = 0 the penalty is an elementwise L2 penalty</span>
<span class="sd">        (aka Frobenius Norm).</span>
<span class="sd">        For l1_ratio = 1 it is an elementwise L1 penalty.</span>
<span class="sd">        For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2.</span>

<span class="sd">    random_state : int, RandomState instance or None, default=None</span>
<span class="sd">        Used for NMF initialisation (when ``init`` == &#39;nndsvdar&#39; or</span>
<span class="sd">        &#39;random&#39;), and in Coordinate Descent. Pass an int for reproducible</span>
<span class="sd">        results across multiple function calls.</span>
<span class="sd">        See :term:`Glossary &lt;random_state&gt;`.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        The verbosity level.</span>

<span class="sd">    shuffle : bool, default=False</span>
<span class="sd">        If true, randomize the order of coordinates in the CD solver.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    W : ndarray of shape (n_samples, n_components)</span>
<span class="sd">        Solution to the non-negative least squares problem.</span>

<span class="sd">    H : ndarray of shape (n_components, n_features)</span>
<span class="sd">        Solution to the non-negative least squares problem.</span>

<span class="sd">    n_iter : int</span>
<span class="sd">        Actual number of iterations.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] :doi:`&quot;Fast local algorithms for large scale nonnegative matrix and tensor</span>
<span class="sd">       factorizations&quot; &lt;10.1587/transfun.E92.A.708&gt;`</span>
<span class="sd">       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals</span>
<span class="sd">       of electronics, communications and computer sciences 92.3: 708-721, 2009.</span>

<span class="sd">    .. [2] :doi:`&quot;Algorithms for nonnegative matrix factorization with the</span>
<span class="sd">       beta-divergence&quot; &lt;10.1162/NECO_a_00168&gt;`</span>
<span class="sd">       Fevotte, C., &amp; Idier, J. (2011). Neural Computation, 23(9).</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.decomposition import non_negative_factorization</span>
<span class="sd">    &gt;&gt;&gt; W, H, n_iter = non_negative_factorization(</span>
<span class="sd">    ...     X, n_components=2, init=&#39;random&#39;, random_state=0)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">est</span> <span class="o">=</span> <span class="n">NMF</span><span class="p">(</span>
        <span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
        <span class="n">solver</span><span class="o">=</span><span class="n">solver</span><span class="p">,</span>
        <span class="n">beta_loss</span><span class="o">=</span><span class="n">beta_loss</span><span class="p">,</span>
        <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="n">alpha_W</span><span class="o">=</span><span class="n">alpha_W</span><span class="p">,</span>
        <span class="n">alpha_H</span><span class="o">=</span><span class="n">alpha_H</span><span class="p">,</span>
        <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">est</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">()</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;csr&quot;</span><span class="p">,</span> <span class="s2">&quot;csc&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">])</span>

    <span class="k">with</span> <span class="n">config_context</span><span class="p">(</span><span class="n">assume_finite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">_fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="n">H</span><span class="p">,</span> <span class="n">update_H</span><span class="o">=</span><span class="n">update_H</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">n_iter</span>


<span class="k">class</span> <span class="nc">_BaseNMF</span><span class="p">(</span><span class="n">ClassNamePrefixFeaturesOutMixin</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Base class for NMF and MiniBatchNMF.&quot;&quot;&quot;</span>

    <span class="c1"># This prevents ``set_split_inverse_transform`` to be generated for the</span>
    <span class="c1"># non-standard ``Xt`` arg on ``inverse_transform``.</span>
    <span class="c1"># TODO(1.7): remove when Xt is removed in v1.7 for inverse_transform</span>
    <span class="n">__metadata_request__inverse_transform</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Xt&quot;</span><span class="p">:</span> <span class="n">metadata_routing</span><span class="o">.</span><span class="n">UNUSED</span><span class="p">}</span>

    <span class="n">_parameter_constraints</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;n_components&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">Interval</span><span class="p">(</span><span class="n">Integral</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">),</span>
            <span class="kc">None</span><span class="p">,</span>
            <span class="n">StrOptions</span><span class="p">({</span><span class="s2">&quot;auto&quot;</span><span class="p">}),</span>
            <span class="n">Hidden</span><span class="p">(</span><span class="n">StrOptions</span><span class="p">({</span><span class="s2">&quot;warn&quot;</span><span class="p">})),</span>
        <span class="p">],</span>
        <span class="s2">&quot;init&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">StrOptions</span><span class="p">({</span><span class="s2">&quot;random&quot;</span><span class="p">,</span> <span class="s2">&quot;nndsvd&quot;</span><span class="p">,</span> <span class="s2">&quot;nndsvda&quot;</span><span class="p">,</span> <span class="s2">&quot;nndsvdar&quot;</span><span class="p">,</span> <span class="s2">&quot;custom&quot;</span><span class="p">}),</span>
            <span class="kc">None</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="s2">&quot;beta_loss&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">StrOptions</span><span class="p">({</span><span class="s2">&quot;frobenius&quot;</span><span class="p">,</span> <span class="s2">&quot;kullback-leibler&quot;</span><span class="p">,</span> <span class="s2">&quot;itakura-saito&quot;</span><span class="p">}),</span>
            <span class="n">Real</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="s2">&quot;tol&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">Interval</span><span class="p">(</span><span class="n">Real</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)],</span>
        <span class="s2">&quot;max_iter&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">Interval</span><span class="p">(</span><span class="n">Integral</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)],</span>
        <span class="s2">&quot;random_state&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;random_state&quot;</span><span class="p">],</span>
        <span class="s2">&quot;alpha_W&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">Interval</span><span class="p">(</span><span class="n">Real</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)],</span>
        <span class="s2">&quot;alpha_H&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">Interval</span><span class="p">(</span><span class="n">Real</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">),</span> <span class="n">StrOptions</span><span class="p">({</span><span class="s2">&quot;same&quot;</span><span class="p">})],</span>
        <span class="s2">&quot;l1_ratio&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">Interval</span><span class="p">(</span><span class="n">Real</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">)],</span>
        <span class="s2">&quot;verbose&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;verbose&quot;</span><span class="p">],</span>
    <span class="p">}</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_components</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">beta_loss</span><span class="o">=</span><span class="s2">&quot;frobenius&quot;</span><span class="p">,</span>
        <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha_W</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">alpha_H</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">n_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta_loss</span> <span class="o">=</span> <span class="n">beta_loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_W</span> <span class="o">=</span> <span class="n">alpha_W</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_H</span> <span class="o">=</span> <span class="n">alpha_H</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="o">=</span> <span class="n">l1_ratio</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

    <span class="k">def</span> <span class="nf">_check_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># n_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">==</span> <span class="s2">&quot;warn&quot;</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="s2">&quot;The default value of `n_components` will change from `None` to&quot;</span>
                    <span class="s2">&quot; `&#39;auto&#39;` in 1.6. Set the value of `n_components` to `None`&quot;</span>
                    <span class="s2">&quot; explicitly to suppress the warning.&quot;</span>
                <span class="p">),</span>
                <span class="ne">FutureWarning</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Keeping the old default value</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># beta_loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span> <span class="o">=</span> <span class="n">_beta_loss_to_float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta_loss</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_w_h</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">update_H</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check W and H, or initialize them.&quot;&quot;&quot;</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">==</span> <span class="s2">&quot;custom&quot;</span> <span class="ow">and</span> <span class="n">update_H</span><span class="p">:</span>
            <span class="n">_check_init</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">),</span> <span class="s2">&quot;NMF (input H)&quot;</span><span class="p">)</span>
            <span class="n">_check_init</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span><span class="p">),</span> <span class="s2">&quot;NMF (input W)&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span> <span class="o">=</span> <span class="n">H</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">H</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">or</span> <span class="n">W</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;H and W should have the same dtype as X. Got &quot;</span>
                    <span class="s2">&quot;H.dtype = </span><span class="si">{}</span><span class="s2"> and W.dtype = </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">H</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="p">)</span>

        <span class="k">elif</span> <span class="ow">not</span> <span class="n">update_H</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;When update_H=False, the provided initial W is not used.&quot;</span><span class="p">,</span>
                    <span class="ne">RuntimeWarning</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="n">_check_init</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">),</span> <span class="s2">&quot;NMF (input H)&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span> <span class="o">=</span> <span class="n">H</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">H</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;H should have the same dtype as X. Got H.dtype = </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">H</span><span class="o">.</span><span class="n">dtype</span>
                    <span class="p">)</span>
                <span class="p">)</span>

            <span class="c1"># &#39;mu&#39; solver should not be initialized by zeros</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;mu&quot;</span><span class="p">:</span>
                <span class="n">avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span><span class="p">)</span>
                <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span><span class="p">),</span> <span class="n">avg</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">H</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="p">(</span>
                        <span class="s2">&quot;When init!=&#39;custom&#39;, provided W or H are ignored. Set &quot;</span>
                        <span class="s2">&quot; init=&#39;custom&#39; to use them as initialization.&quot;</span>
                    <span class="p">),</span>
                    <span class="ne">RuntimeWarning</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">W</span><span class="p">,</span> <span class="n">H</span> <span class="o">=</span> <span class="n">_initialize_nmf</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span>

    <span class="k">def</span> <span class="nf">_compute_regularization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute scaled regularization terms.&quot;&quot;&quot;</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">alpha_W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha_W</span>
        <span class="n">alpha_H</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha_W</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha_H</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha_H</span>

        <span class="n">l1_reg_W</span> <span class="o">=</span> <span class="n">n_features</span> <span class="o">*</span> <span class="n">alpha_W</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span>
        <span class="n">l1_reg_H</span> <span class="o">=</span> <span class="n">n_samples</span> <span class="o">*</span> <span class="n">alpha_H</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span>
        <span class="n">l2_reg_W</span> <span class="o">=</span> <span class="n">n_features</span> <span class="o">*</span> <span class="n">alpha_W</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">)</span>
        <span class="n">l2_reg_H</span> <span class="o">=</span> <span class="n">n_samples</span> <span class="o">*</span> <span class="n">alpha_H</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">l1_reg_W</span><span class="p">,</span> <span class="n">l1_reg_H</span><span class="p">,</span> <span class="n">l2_reg_W</span><span class="p">,</span> <span class="n">l2_reg_H</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Learn a NMF model for the data X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Training vector, where `n_samples` is the number of samples</span>
<span class="sd">            and `n_features` is the number of features.</span>

<span class="sd">        y : Ignored</span>
<span class="sd">            Not used, present for API consistency by convention.</span>

<span class="sd">        **params : kwargs</span>
<span class="sd">            Parameters (keyword arguments) and values passed to</span>
<span class="sd">            the fit_transform instance.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns the instance itself.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># param validation is done in fit_transform</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">Xt</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Transform data back to its original space.</span>

<span class="sd">        .. versionadded:: 0.18</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {ndarray, sparse matrix} of shape (n_samples, n_components)</span>
<span class="sd">            Transformed data matrix.</span>

<span class="sd">        Xt : {ndarray, sparse matrix} of shape (n_samples, n_components)</span>
<span class="sd">            Transformed data matrix.</span>

<span class="sd">            .. deprecated:: 1.5</span>
<span class="sd">                `Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X : ndarray of shape (n_samples, n_features)</span>
<span class="sd">            Returns a data matrix of the original shape.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">X</span> <span class="o">=</span> <span class="n">_deprecate_Xt_in_inverse_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Xt</span><span class="p">)</span>

        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">components_</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_n_features_out</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of transformed output features.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;requires_positive_X&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s2">&quot;preserves_dtype&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span>
        <span class="p">}</span>


<span class="k">class</span> <span class="nc">NMF</span><span class="p">(</span><span class="n">_BaseNMF</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Non-Negative Matrix Factorization (NMF).</span>

<span class="sd">    Find two non-negative matrices, i.e. matrices with all non-negative elements, (W, H)</span>
<span class="sd">    whose product approximates the non-negative matrix X. This factorization can be used</span>
<span class="sd">    for example for dimensionality reduction, source separation or topic extraction.</span>

<span class="sd">    The objective function is:</span>

<span class="sd">        .. math::</span>

<span class="sd">            L(W, H) &amp;= 0.5 * ||X - WH||_{loss}^2</span>

<span class="sd">            &amp;+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1</span>

<span class="sd">            &amp;+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1</span>

<span class="sd">            &amp;+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2</span>

<span class="sd">            &amp;+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2</span>

<span class="sd">    Where:</span>

<span class="sd">    :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)</span>

<span class="sd">    :math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)</span>

<span class="sd">    The generic norm :math:`||X - WH||_{loss}` may represent</span>
<span class="sd">    the Frobenius norm or another supported beta-divergence loss.</span>
<span class="sd">    The choice between options is controlled by the `beta_loss` parameter.</span>

<span class="sd">    The regularization terms are scaled by `n_features` for `W` and by `n_samples` for</span>
<span class="sd">    `H` to keep their impact balanced with respect to one another and to the data fit</span>
<span class="sd">    term as independent as possible of the size `n_samples` of the training set.</span>

<span class="sd">    The objective function is minimized with an alternating minimization of W</span>
<span class="sd">    and H.</span>

<span class="sd">    Note that the transformed data is named W and the components matrix is named H. In</span>
<span class="sd">    the NMF literature, the naming convention is usually the opposite since the data</span>
<span class="sd">    matrix X is transposed.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;NMF&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_components : int or {&#39;auto&#39;} or None, default=None</span>
<span class="sd">        Number of components, if n_components is not set all features</span>
<span class="sd">        are kept.</span>
<span class="sd">        If `n_components=&#39;auto&#39;`, the number of components is automatically inferred</span>
<span class="sd">        from W or H shapes.</span>

<span class="sd">        .. versionchanged:: 1.4</span>
<span class="sd">            Added `&#39;auto&#39;` value.</span>

<span class="sd">    init : {&#39;random&#39;, &#39;nndsvd&#39;, &#39;nndsvda&#39;, &#39;nndsvdar&#39;, &#39;custom&#39;}, default=None</span>
<span class="sd">        Method used to initialize the procedure.</span>
<span class="sd">        Valid options:</span>

<span class="sd">        - `None`: &#39;nndsvda&#39; if n_components &lt;= min(n_samples, n_features),</span>
<span class="sd">          otherwise random.</span>

<span class="sd">        - `&#39;random&#39;`: non-negative random matrices, scaled with:</span>
<span class="sd">          `sqrt(X.mean() / n_components)`</span>

<span class="sd">        - `&#39;nndsvd&#39;`: Nonnegative Double Singular Value Decomposition (NNDSVD)</span>
<span class="sd">          initialization (better for sparseness)</span>

<span class="sd">        - `&#39;nndsvda&#39;`: NNDSVD with zeros filled with the average of X</span>
<span class="sd">          (better when sparsity is not desired)</span>

<span class="sd">        - `&#39;nndsvdar&#39;` NNDSVD with zeros filled with small random values</span>
<span class="sd">          (generally faster, less accurate alternative to NNDSVDa</span>
<span class="sd">          for when sparsity is not desired)</span>

<span class="sd">        - `&#39;custom&#39;`: Use custom matrices `W` and `H` which must both be provided.</span>

<span class="sd">        .. versionchanged:: 1.1</span>
<span class="sd">            When `init=None` and n_components is less than n_samples and n_features</span>
<span class="sd">            defaults to `nndsvda` instead of `nndsvd`.</span>

<span class="sd">    solver : {&#39;cd&#39;, &#39;mu&#39;}, default=&#39;cd&#39;</span>
<span class="sd">        Numerical solver to use:</span>

<span class="sd">        - &#39;cd&#39; is a Coordinate Descent solver.</span>
<span class="sd">        - &#39;mu&#39; is a Multiplicative Update solver.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           Coordinate Descent solver.</span>

<span class="sd">        .. versionadded:: 0.19</span>
<span class="sd">           Multiplicative Update solver.</span>

<span class="sd">    beta_loss : float or {&#39;frobenius&#39;, &#39;kullback-leibler&#39;, \</span>
<span class="sd">            &#39;itakura-saito&#39;}, default=&#39;frobenius&#39;</span>
<span class="sd">        Beta divergence to be minimized, measuring the distance between X</span>
<span class="sd">        and the dot product WH. Note that values different from &#39;frobenius&#39;</span>
<span class="sd">        (or 2) and &#39;kullback-leibler&#39; (or 1) lead to significantly slower</span>
<span class="sd">        fits. Note that for beta_loss &lt;= 0 (or &#39;itakura-saito&#39;), the input</span>
<span class="sd">        matrix X cannot contain zeros. Used only in &#39;mu&#39; solver.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    tol : float, default=1e-4</span>
<span class="sd">        Tolerance of the stopping condition.</span>

<span class="sd">    max_iter : int, default=200</span>
<span class="sd">        Maximum number of iterations before timing out.</span>

<span class="sd">    random_state : int, RandomState instance or None, default=None</span>
<span class="sd">        Used for initialisation (when ``init`` == &#39;nndsvdar&#39; or</span>
<span class="sd">        &#39;random&#39;), and in Coordinate Descent. Pass an int for reproducible</span>
<span class="sd">        results across multiple function calls.</span>
<span class="sd">        See :term:`Glossary &lt;random_state&gt;`.</span>

<span class="sd">    alpha_W : float, default=0.0</span>
<span class="sd">        Constant that multiplies the regularization terms of `W`. Set it to zero</span>
<span class="sd">        (default) to have no regularization on `W`.</span>

<span class="sd">        .. versionadded:: 1.0</span>

<span class="sd">    alpha_H : float or &quot;same&quot;, default=&quot;same&quot;</span>
<span class="sd">        Constant that multiplies the regularization terms of `H`. Set it to zero to</span>
<span class="sd">        have no regularization on `H`. If &quot;same&quot; (default), it takes the same value as</span>
<span class="sd">        `alpha_W`.</span>

<span class="sd">        .. versionadded:: 1.0</span>

<span class="sd">    l1_ratio : float, default=0.0</span>
<span class="sd">        The regularization mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.</span>
<span class="sd">        For l1_ratio = 0 the penalty is an elementwise L2 penalty</span>
<span class="sd">        (aka Frobenius Norm).</span>
<span class="sd">        For l1_ratio = 1 it is an elementwise L1 penalty.</span>
<span class="sd">        For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           Regularization parameter *l1_ratio* used in the Coordinate Descent</span>
<span class="sd">           solver.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        Whether to be verbose.</span>

<span class="sd">    shuffle : bool, default=False</span>
<span class="sd">        If true, randomize the order of coordinates in the CD solver.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *shuffle* parameter used in the Coordinate Descent solver.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    components_ : ndarray of shape (n_components, n_features)</span>
<span class="sd">        Factorization matrix, sometimes called &#39;dictionary&#39;.</span>

<span class="sd">    n_components_ : int</span>
<span class="sd">        The number of components. It is same as the `n_components` parameter</span>
<span class="sd">        if it was given. Otherwise, it will be same as the number of</span>
<span class="sd">        features.</span>

<span class="sd">    reconstruction_err_ : float</span>
<span class="sd">        Frobenius norm of the matrix difference, or beta-divergence, between</span>
<span class="sd">        the training data ``X`` and the reconstructed data ``WH`` from</span>
<span class="sd">        the fitted model.</span>

<span class="sd">    n_iter_ : int</span>
<span class="sd">        Actual number of iterations.</span>

<span class="sd">    n_features_in_ : int</span>
<span class="sd">        Number of features seen during :term:`fit`.</span>

<span class="sd">        .. versionadded:: 0.24</span>

<span class="sd">    feature_names_in_ : ndarray of shape (`n_features_in_`,)</span>
<span class="sd">        Names of features seen during :term:`fit`. Defined only when `X`</span>
<span class="sd">        has feature names that are all strings.</span>

<span class="sd">        .. versionadded:: 1.0</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    DictionaryLearning : Find a dictionary that sparsely encodes data.</span>
<span class="sd">    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.</span>
<span class="sd">    PCA : Principal component analysis.</span>
<span class="sd">    SparseCoder : Find a sparse representation of data from a fixed,</span>
<span class="sd">        precomputed dictionary.</span>
<span class="sd">    SparsePCA : Sparse Principal Components Analysis.</span>
<span class="sd">    TruncatedSVD : Dimensionality reduction using truncated SVD.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] :doi:`&quot;Fast local algorithms for large scale nonnegative matrix and tensor</span>
<span class="sd">       factorizations&quot; &lt;10.1587/transfun.E92.A.708&gt;`</span>
<span class="sd">       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals</span>
<span class="sd">       of electronics, communications and computer sciences 92.3: 708-721, 2009.</span>

<span class="sd">    .. [2] :doi:`&quot;Algorithms for nonnegative matrix factorization with the</span>
<span class="sd">       beta-divergence&quot; &lt;10.1162/NECO_a_00168&gt;`</span>
<span class="sd">       Fevotte, C., &amp; Idier, J. (2011). Neural Computation, 23(9).</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.decomposition import NMF</span>
<span class="sd">    &gt;&gt;&gt; model = NMF(n_components=2, init=&#39;random&#39;, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; W = model.fit_transform(X)</span>
<span class="sd">    &gt;&gt;&gt; H = model.components_</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_parameter_constraints</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="n">_BaseNMF</span><span class="o">.</span><span class="n">_parameter_constraints</span><span class="p">,</span>
        <span class="s2">&quot;solver&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">StrOptions</span><span class="p">({</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="s2">&quot;cd&quot;</span><span class="p">})],</span>
        <span class="s2">&quot;shuffle&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;boolean&quot;</span><span class="p">],</span>
    <span class="p">}</span>

<div class="viewcode-block" id="NMF.__init__">
<a class="viewcode-back" href="../../../_autosummary/cuvis_ai.preprocessor.sklearn_wrapped.NMF.html#cuvis_ai.preprocessor.sklearn_wrapped.NMF.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_components</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;cd&quot;</span><span class="p">,</span>
        <span class="n">beta_loss</span><span class="o">=</span><span class="s2">&quot;frobenius&quot;</span><span class="p">,</span>
        <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha_W</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">alpha_H</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span>
            <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
            <span class="n">beta_loss</span><span class="o">=</span><span class="n">beta_loss</span><span class="p">,</span>
            <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">alpha_W</span><span class="o">=</span><span class="n">alpha_W</span><span class="p">,</span>
            <span class="n">alpha_H</span><span class="o">=</span><span class="n">alpha_H</span><span class="p">,</span>
            <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">=</span> <span class="n">solver</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span> <span class="o">=</span> <span class="n">shuffle</span></div>


    <span class="k">def</span> <span class="nf">_check_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_check_params</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># solver</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">!=</span> <span class="s2">&quot;mu&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_loss</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;frobenius&quot;</span><span class="p">):</span>
            <span class="c1"># &#39;mu&#39; is the only solver that handles other beta losses than &#39;frobenius&#39;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Invalid beta_loss parameter: solver </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="si">!r}</span><span class="s2"> does not handle &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;beta_loss = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">beta_loss</span><span class="si">!r}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;mu&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">==</span> <span class="s2">&quot;nndsvd&quot;</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="s2">&quot;The multiplicative update (&#39;mu&#39;) solver cannot update &quot;</span>
                    <span class="s2">&quot;zeros present in the initialization, and so leads to &quot;</span>
                    <span class="s2">&quot;poorer results when used jointly with init=&#39;nndsvd&#39;. &quot;</span>
                    <span class="s2">&quot;You may try init=&#39;nndsvda&#39; or init=&#39;nndsvdar&#39; instead.&quot;</span>
                <span class="p">),</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@_fit_context</span><span class="p">(</span><span class="n">prefer_skip_nested_validation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Learn a NMF model for the data X and returns the transformed data.</span>

<span class="sd">        This is more efficient than calling fit followed by transform.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Training vector, where `n_samples` is the number of samples</span>
<span class="sd">            and `n_features` is the number of features.</span>

<span class="sd">        y : Ignored</span>
<span class="sd">            Not used, present for API consistency by convention.</span>

<span class="sd">        W : array-like of shape (n_samples, n_components), default=None</span>
<span class="sd">            If `init=&#39;custom&#39;`, it is used as initial guess for the solution.</span>
<span class="sd">            If `None`, uses the initialisation method specified in `init`.</span>

<span class="sd">        H : array-like of shape (n_components, n_features), default=None</span>
<span class="sd">            If `init=&#39;custom&#39;`, it is used as initial guess for the solution.</span>
<span class="sd">            If `None`, uses the initialisation method specified in `init`.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        W : ndarray of shape (n_samples, n_components)</span>
<span class="sd">            Transformed data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;csr&quot;</span><span class="p">,</span> <span class="s2">&quot;csc&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="k">with</span> <span class="n">config_context</span><span class="p">(</span><span class="n">assume_finite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="n">H</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_err_</span> <span class="o">=</span> <span class="n">_beta_divergence</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span><span class="p">,</span> <span class="n">square_root</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_components_</span> <span class="o">=</span> <span class="n">H</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">components_</span> <span class="o">=</span> <span class="n">H</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">n_iter</span>

        <span class="k">return</span> <span class="n">W</span>

    <span class="k">def</span> <span class="nf">_fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">update_H</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Learn a NMF model for the data X and returns the transformed data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Data matrix to be decomposed</span>

<span class="sd">        y : Ignored</span>

<span class="sd">        W : array-like of shape (n_samples, n_components), default=None</span>
<span class="sd">            If `init=&#39;custom&#39;`, it is used as initial guess for the solution.</span>
<span class="sd">            If `update_H=False`, it is initialised as an array of zeros, unless</span>
<span class="sd">            `solver=&#39;mu&#39;`, then it is filled with values calculated by</span>
<span class="sd">            `np.sqrt(X.mean() / self._n_components)`.</span>
<span class="sd">            If `None`, uses the initialisation method specified in `init`.</span>

<span class="sd">        H : array-like of shape (n_components, n_features), default=None</span>
<span class="sd">            If `init=&#39;custom&#39;`, it is used as initial guess for the solution.</span>
<span class="sd">            If `update_H=False`, it is used as a constant, to solve for W only.</span>
<span class="sd">            If `None`, uses the initialisation method specified in `init`.</span>

<span class="sd">        update_H : bool, default=True</span>
<span class="sd">            If True, both W and H will be estimated from initial guesses,</span>
<span class="sd">            this corresponds to a call to the &#39;fit_transform&#39; method.</span>
<span class="sd">            If False, only W will be estimated, this corresponds to a call</span>
<span class="sd">            to the &#39;transform&#39; method.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        W : ndarray of shape (n_samples, n_components)</span>
<span class="sd">            Transformed data.</span>

<span class="sd">        H : ndarray of shape (n_components, n_features)</span>
<span class="sd">            Factorization matrix, sometimes called &#39;dictionary&#39;.</span>

<span class="sd">        n_iter_ : int</span>
<span class="sd">            Actual number of iterations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_non_negative</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s2">&quot;NMF (input X)&quot;</span><span class="p">)</span>

        <span class="c1"># check parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_params</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;When beta_loss &lt;= 0 and X contains zeros, &quot;</span>
                <span class="s2">&quot;the solver may diverge. Please add small values &quot;</span>
                <span class="s2">&quot;to X, or use a positive beta_loss.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># initialize or check W and H</span>
        <span class="n">W</span><span class="p">,</span> <span class="n">H</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_w_h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">update_H</span><span class="p">)</span>

        <span class="c1"># scale the regularization terms</span>
        <span class="n">l1_reg_W</span><span class="p">,</span> <span class="n">l1_reg_H</span><span class="p">,</span> <span class="n">l2_reg_W</span><span class="p">,</span> <span class="n">l2_reg_H</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_regularization</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;cd&quot;</span><span class="p">:</span>
            <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="n">_fit_coordinate_descent</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span>
                <span class="n">W</span><span class="p">,</span>
                <span class="n">H</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span>
                <span class="n">l1_reg_W</span><span class="p">,</span>
                <span class="n">l1_reg_H</span><span class="p">,</span>
                <span class="n">l2_reg_W</span><span class="p">,</span>
                <span class="n">l2_reg_H</span><span class="p">,</span>
                <span class="n">update_H</span><span class="o">=</span><span class="n">update_H</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                <span class="n">shuffle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;mu&quot;</span><span class="p">:</span>
            <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">_fit_multiplicative_update</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span>
                <span class="n">W</span><span class="p">,</span>
                <span class="n">H</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
                <span class="n">l1_reg_W</span><span class="p">,</span>
                <span class="n">l1_reg_H</span><span class="p">,</span>
                <span class="n">l2_reg_W</span><span class="p">,</span>
                <span class="n">l2_reg_H</span><span class="p">,</span>
                <span class="n">update_H</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid solver parameter &#39;</span><span class="si">%s</span><span class="s2">&#39;.&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_iter</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Maximum number of iterations </span><span class="si">%d</span><span class="s2"> reached. Increase &quot;</span>
                <span class="s2">&quot;it to improve convergence.&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span>
                <span class="n">ConvergenceWarning</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">n_iter</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Transform the data X according to the fitted NMF model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Training vector, where `n_samples` is the number of samples</span>
<span class="sd">            and `n_features` is the number of features.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        W : ndarray of shape (n_samples, n_components)</span>
<span class="sd">            Transformed data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;csr&quot;</span><span class="p">,</span> <span class="s2">&quot;csc&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="n">reset</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="k">with</span> <span class="n">config_context</span><span class="p">(</span><span class="n">assume_finite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">W</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> <span class="n">update_H</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">W</span>


<span class="k">class</span> <span class="nc">MiniBatchNMF</span><span class="p">(</span><span class="n">_BaseNMF</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mini-Batch Non-Negative Matrix Factorization (NMF).</span>

<span class="sd">    .. versionadded:: 1.1</span>

<span class="sd">    Find two non-negative matrices, i.e. matrices with all non-negative elements,</span>
<span class="sd">    (`W`, `H`) whose product approximates the non-negative matrix `X`. This</span>
<span class="sd">    factorization can be used for example for dimensionality reduction, source</span>
<span class="sd">    separation or topic extraction.</span>

<span class="sd">    The objective function is:</span>

<span class="sd">        .. math::</span>

<span class="sd">            L(W, H) &amp;= 0.5 * ||X - WH||_{loss}^2</span>

<span class="sd">            &amp;+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1</span>

<span class="sd">            &amp;+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1</span>

<span class="sd">            &amp;+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2</span>

<span class="sd">            &amp;+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2</span>

<span class="sd">    Where:</span>

<span class="sd">    :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)</span>

<span class="sd">    :math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)</span>

<span class="sd">    The generic norm :math:`||X - WH||_{loss}^2` may represent</span>
<span class="sd">    the Frobenius norm or another supported beta-divergence loss.</span>
<span class="sd">    The choice between options is controlled by the `beta_loss` parameter.</span>

<span class="sd">    The objective function is minimized with an alternating minimization of `W`</span>
<span class="sd">    and `H`.</span>

<span class="sd">    Note that the transformed data is named `W` and the components matrix is</span>
<span class="sd">    named `H`. In the NMF literature, the naming convention is usually the opposite</span>
<span class="sd">    since the data matrix `X` is transposed.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;MiniBatchNMF&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_components : int or {&#39;auto&#39;} or None, default=None</span>
<span class="sd">        Number of components, if `n_components` is not set all features</span>
<span class="sd">        are kept.</span>
<span class="sd">        If `n_components=&#39;auto&#39;`, the number of components is automatically inferred</span>
<span class="sd">        from W or H shapes.</span>

<span class="sd">        .. versionchanged:: 1.4</span>
<span class="sd">            Added `&#39;auto&#39;` value.</span>

<span class="sd">    init : {&#39;random&#39;, &#39;nndsvd&#39;, &#39;nndsvda&#39;, &#39;nndsvdar&#39;, &#39;custom&#39;}, default=None</span>
<span class="sd">        Method used to initialize the procedure.</span>
<span class="sd">        Valid options:</span>

<span class="sd">        - `None`: &#39;nndsvda&#39; if `n_components &lt;= min(n_samples, n_features)`,</span>
<span class="sd">          otherwise random.</span>

<span class="sd">        - `&#39;random&#39;`: non-negative random matrices, scaled with:</span>
<span class="sd">          `sqrt(X.mean() / n_components)`</span>

<span class="sd">        - `&#39;nndsvd&#39;`: Nonnegative Double Singular Value Decomposition (NNDSVD)</span>
<span class="sd">          initialization (better for sparseness).</span>

<span class="sd">        - `&#39;nndsvda&#39;`: NNDSVD with zeros filled with the average of X</span>
<span class="sd">          (better when sparsity is not desired).</span>

<span class="sd">        - `&#39;nndsvdar&#39;` NNDSVD with zeros filled with small random values</span>
<span class="sd">          (generally faster, less accurate alternative to NNDSVDa</span>
<span class="sd">          for when sparsity is not desired).</span>

<span class="sd">        - `&#39;custom&#39;`: Use custom matrices `W` and `H` which must both be provided.</span>

<span class="sd">    batch_size : int, default=1024</span>
<span class="sd">        Number of samples in each mini-batch. Large batch sizes</span>
<span class="sd">        give better long-term convergence at the cost of a slower start.</span>

<span class="sd">    beta_loss : float or {&#39;frobenius&#39;, &#39;kullback-leibler&#39;, \</span>
<span class="sd">            &#39;itakura-saito&#39;}, default=&#39;frobenius&#39;</span>
<span class="sd">        Beta divergence to be minimized, measuring the distance between `X`</span>
<span class="sd">        and the dot product `WH`. Note that values different from &#39;frobenius&#39;</span>
<span class="sd">        (or 2) and &#39;kullback-leibler&#39; (or 1) lead to significantly slower</span>
<span class="sd">        fits. Note that for `beta_loss &lt;= 0` (or &#39;itakura-saito&#39;), the input</span>
<span class="sd">        matrix `X` cannot contain zeros.</span>

<span class="sd">    tol : float, default=1e-4</span>
<span class="sd">        Control early stopping based on the norm of the differences in `H`</span>
<span class="sd">        between 2 steps. To disable early stopping based on changes in `H`, set</span>
<span class="sd">        `tol` to 0.0.</span>

<span class="sd">    max_no_improvement : int, default=10</span>
<span class="sd">        Control early stopping based on the consecutive number of mini batches</span>
<span class="sd">        that does not yield an improvement on the smoothed cost function.</span>
<span class="sd">        To disable convergence detection based on cost function, set</span>
<span class="sd">        `max_no_improvement` to None.</span>

<span class="sd">    max_iter : int, default=200</span>
<span class="sd">        Maximum number of iterations over the complete dataset before</span>
<span class="sd">        timing out.</span>

<span class="sd">    alpha_W : float, default=0.0</span>
<span class="sd">        Constant that multiplies the regularization terms of `W`. Set it to zero</span>
<span class="sd">        (default) to have no regularization on `W`.</span>

<span class="sd">    alpha_H : float or &quot;same&quot;, default=&quot;same&quot;</span>
<span class="sd">        Constant that multiplies the regularization terms of `H`. Set it to zero to</span>
<span class="sd">        have no regularization on `H`. If &quot;same&quot; (default), it takes the same value as</span>
<span class="sd">        `alpha_W`.</span>

<span class="sd">    l1_ratio : float, default=0.0</span>
<span class="sd">        The regularization mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.</span>
<span class="sd">        For l1_ratio = 0 the penalty is an elementwise L2 penalty</span>
<span class="sd">        (aka Frobenius Norm).</span>
<span class="sd">        For l1_ratio = 1 it is an elementwise L1 penalty.</span>
<span class="sd">        For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2.</span>

<span class="sd">    forget_factor : float, default=0.7</span>
<span class="sd">        Amount of rescaling of past information. Its value could be 1 with</span>
<span class="sd">        finite datasets. Choosing values &lt; 1 is recommended with online</span>
<span class="sd">        learning as more recent batches will weight more than past batches.</span>

<span class="sd">    fresh_restarts : bool, default=False</span>
<span class="sd">        Whether to completely solve for W at each step. Doing fresh restarts will likely</span>
<span class="sd">        lead to a better solution for a same number of iterations but it is much slower.</span>

<span class="sd">    fresh_restarts_max_iter : int, default=30</span>
<span class="sd">        Maximum number of iterations when solving for W at each step. Only used when</span>
<span class="sd">        doing fresh restarts. These iterations may be stopped early based on a small</span>
<span class="sd">        change of W controlled by `tol`.</span>

<span class="sd">    transform_max_iter : int, default=None</span>
<span class="sd">        Maximum number of iterations when solving for W at transform time.</span>
<span class="sd">        If None, it defaults to `max_iter`.</span>

<span class="sd">    random_state : int, RandomState instance or None, default=None</span>
<span class="sd">        Used for initialisation (when ``init`` == &#39;nndsvdar&#39; or</span>
<span class="sd">        &#39;random&#39;), and in Coordinate Descent. Pass an int for reproducible</span>
<span class="sd">        results across multiple function calls.</span>
<span class="sd">        See :term:`Glossary &lt;random_state&gt;`.</span>

<span class="sd">    verbose : bool, default=False</span>
<span class="sd">        Whether to be verbose.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    components_ : ndarray of shape (n_components, n_features)</span>
<span class="sd">        Factorization matrix, sometimes called &#39;dictionary&#39;.</span>

<span class="sd">    n_components_ : int</span>
<span class="sd">        The number of components. It is same as the `n_components` parameter</span>
<span class="sd">        if it was given. Otherwise, it will be same as the number of</span>
<span class="sd">        features.</span>

<span class="sd">    reconstruction_err_ : float</span>
<span class="sd">        Frobenius norm of the matrix difference, or beta-divergence, between</span>
<span class="sd">        the training data `X` and the reconstructed data `WH` from</span>
<span class="sd">        the fitted model.</span>

<span class="sd">    n_iter_ : int</span>
<span class="sd">        Actual number of started iterations over the whole dataset.</span>

<span class="sd">    n_steps_ : int</span>
<span class="sd">        Number of mini-batches processed.</span>

<span class="sd">    n_features_in_ : int</span>
<span class="sd">        Number of features seen during :term:`fit`.</span>

<span class="sd">    feature_names_in_ : ndarray of shape (`n_features_in_`,)</span>
<span class="sd">        Names of features seen during :term:`fit`. Defined only when `X`</span>
<span class="sd">        has feature names that are all strings.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    NMF : Non-negative matrix factorization.</span>
<span class="sd">    MiniBatchDictionaryLearning : Finds a dictionary that can best be used to represent</span>
<span class="sd">        data using a sparse code.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] :doi:`&quot;Fast local algorithms for large scale nonnegative matrix and tensor</span>
<span class="sd">       factorizations&quot; &lt;10.1587/transfun.E92.A.708&gt;`</span>
<span class="sd">       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals</span>
<span class="sd">       of electronics, communications and computer sciences 92.3: 708-721, 2009.</span>

<span class="sd">    .. [2] :doi:`&quot;Algorithms for nonnegative matrix factorization with the</span>
<span class="sd">       beta-divergence&quot; &lt;10.1162/NECO_a_00168&gt;`</span>
<span class="sd">       Fevotte, C., &amp; Idier, J. (2011). Neural Computation, 23(9).</span>

<span class="sd">    .. [3] :doi:`&quot;Online algorithms for nonnegative matrix factorization with the</span>
<span class="sd">       Itakura-Saito divergence&quot; &lt;10.1109/ASPAA.2011.6082314&gt;`</span>
<span class="sd">       Lefevre, A., Bach, F., Fevotte, C. (2011). WASPA.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.decomposition import MiniBatchNMF</span>
<span class="sd">    &gt;&gt;&gt; model = MiniBatchNMF(n_components=2, init=&#39;random&#39;, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; W = model.fit_transform(X)</span>
<span class="sd">    &gt;&gt;&gt; H = model.components_</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_parameter_constraints</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="n">_BaseNMF</span><span class="o">.</span><span class="n">_parameter_constraints</span><span class="p">,</span>
        <span class="s2">&quot;max_no_improvement&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">Interval</span><span class="p">(</span><span class="n">Integral</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">),</span> <span class="kc">None</span><span class="p">],</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">Interval</span><span class="p">(</span><span class="n">Integral</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)],</span>
        <span class="s2">&quot;forget_factor&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">Interval</span><span class="p">(</span><span class="n">Real</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">)],</span>
        <span class="s2">&quot;fresh_restarts&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;boolean&quot;</span><span class="p">],</span>
        <span class="s2">&quot;fresh_restarts_max_iter&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">Interval</span><span class="p">(</span><span class="n">Integral</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)],</span>
        <span class="s2">&quot;transform_max_iter&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">Interval</span><span class="p">(</span><span class="n">Integral</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">closed</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">),</span> <span class="kc">None</span><span class="p">],</span>
    <span class="p">}</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_components</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">beta_loss</span><span class="o">=</span><span class="s2">&quot;frobenius&quot;</span><span class="p">,</span>
        <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">max_no_improvement</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
        <span class="n">alpha_W</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">alpha_H</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">forget_factor</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
        <span class="n">fresh_restarts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">fresh_restarts_max_iter</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
        <span class="n">transform_max_iter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span>
            <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
            <span class="n">beta_loss</span><span class="o">=</span><span class="n">beta_loss</span><span class="p">,</span>
            <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">alpha_W</span><span class="o">=</span><span class="n">alpha_W</span><span class="p">,</span>
            <span class="n">alpha_H</span><span class="o">=</span><span class="n">alpha_H</span><span class="p">,</span>
            <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_no_improvement</span> <span class="o">=</span> <span class="n">max_no_improvement</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_factor</span> <span class="o">=</span> <span class="n">forget_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fresh_restarts</span> <span class="o">=</span> <span class="n">fresh_restarts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fresh_restarts_max_iter</span> <span class="o">=</span> <span class="n">fresh_restarts_max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform_max_iter</span> <span class="o">=</span> <span class="n">transform_max_iter</span>

    <span class="k">def</span> <span class="nf">_check_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_check_params</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># forget_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_rho</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forget_factor</span> <span class="o">**</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># gamma for Maximization-Minimization (MM) algorithm [Fevotte 2011]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="c1"># transform_max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_transform_max_iter</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform_max_iter</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform_max_iter</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_solve_W</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Minimize the objective function w.r.t W.</span>

<span class="sd">        Update W with H being fixed, until convergence. This is the heart</span>
<span class="sd">        of `transform` but it&#39;s also used during `fit` when doing fresh restarts.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span><span class="p">)</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_components</span><span class="p">),</span> <span class="n">avg</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">W_buffer</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># Get scaled regularization terms. Done for each minibatch to take into account</span>
        <span class="c1"># variable sizes of minibatches.</span>
        <span class="n">l1_reg_W</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">l2_reg_W</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_regularization</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="n">W</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">_multiplicative_update_w</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span><span class="p">,</span> <span class="n">l1_reg_W</span><span class="p">,</span> <span class="n">l2_reg_W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span>
            <span class="p">)</span>

            <span class="n">W_diff</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">W</span> <span class="o">-</span> <span class="n">W_buffer</span><span class="p">)</span> <span class="o">/</span> <span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">W_diff</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">W_buffer</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">W</span>

        <span class="k">return</span> <span class="n">W</span>

    <span class="k">def</span> <span class="nf">_minibatch_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">update_H</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform the update of W and H for one minibatch.&quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># get scaled regularization terms. Done for each minibatch to take into account</span>
        <span class="c1"># variable sizes of minibatches.</span>
        <span class="n">l1_reg_W</span><span class="p">,</span> <span class="n">l1_reg_H</span><span class="p">,</span> <span class="n">l2_reg_W</span><span class="p">,</span> <span class="n">l2_reg_H</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_regularization</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># update W</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fresh_restarts</span> <span class="ow">or</span> <span class="n">W</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_solve_W</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fresh_restarts_max_iter</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">W</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">_multiplicative_update_w</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span><span class="p">,</span> <span class="n">l1_reg_W</span><span class="p">,</span> <span class="n">l2_reg_W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span>
            <span class="p">)</span>

        <span class="c1"># necessary for stability with beta_loss &lt; 1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">W</span><span class="p">[</span><span class="n">W</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="n">batch_cost</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">_beta_divergence</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span><span class="p">)</span>
            <span class="o">+</span> <span class="n">l1_reg_W</span> <span class="o">*</span> <span class="n">W</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="o">+</span> <span class="n">l1_reg_H</span> <span class="o">*</span> <span class="n">H</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="o">+</span> <span class="n">l2_reg_W</span> <span class="o">*</span> <span class="p">(</span><span class="n">W</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="o">+</span> <span class="n">l2_reg_H</span> <span class="o">*</span> <span class="p">(</span><span class="n">H</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span>

        <span class="c1"># update H (only at fit or fit_transform)</span>
        <span class="k">if</span> <span class="n">update_H</span><span class="p">:</span>
            <span class="n">H</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">_multiplicative_update_h</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span>
                <span class="n">W</span><span class="p">,</span>
                <span class="n">H</span><span class="p">,</span>
                <span class="n">beta_loss</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span><span class="p">,</span>
                <span class="n">l1_reg_H</span><span class="o">=</span><span class="n">l1_reg_H</span><span class="p">,</span>
                <span class="n">l2_reg_H</span><span class="o">=</span><span class="n">l2_reg_H</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span><span class="p">,</span>
                <span class="n">A</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_components_numerator</span><span class="p">,</span>
                <span class="n">B</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_components_denominator</span><span class="p">,</span>
                <span class="n">rho</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rho</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># necessary for stability with beta_loss &lt; 1</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">H</span><span class="p">[</span><span class="n">H</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="k">return</span> <span class="n">batch_cost</span>

    <span class="k">def</span> <span class="nf">_minibatch_convergence</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">batch_cost</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">H_buffer</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">n_steps</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Helper function to encapsulate the early stopping logic&quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># counts steps starting from 1 for user friendly verbose mode.</span>
        <span class="n">step</span> <span class="o">=</span> <span class="n">step</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># Ignore first iteration because H is not updated yet.</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Minibatch step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_steps</span><span class="si">}</span><span class="s2">: mean batch cost: </span><span class="si">{</span><span class="n">batch_cost</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># Compute an Exponentially Weighted Average of the cost function to</span>
        <span class="c1"># monitor the convergence while discarding minibatch-local stochastic</span>
        <span class="c1"># variability: https://en.wikipedia.org/wiki/Moving_average</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ewa_cost</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ewa_cost</span> <span class="o">=</span> <span class="n">batch_cost</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_samples</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ewa_cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ewa_cost</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">+</span> <span class="n">batch_cost</span> <span class="o">*</span> <span class="n">alpha</span>

        <span class="c1"># Log progress to be able to monitor convergence</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Minibatch step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_steps</span><span class="si">}</span><span class="s2">: mean batch cost: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">batch_cost</span><span class="si">}</span><span class="s2">, ewa cost: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_ewa_cost</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Early stopping based on change of H</span>
        <span class="n">H_diff</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">H</span> <span class="o">-</span> <span class="n">H_buffer</span><span class="p">)</span> <span class="o">/</span> <span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">H_diff</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Converged (small H change) at step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_steps</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">True</span>

        <span class="c1"># Early stopping heuristic due to lack of improvement on smoothed</span>
        <span class="c1"># cost function</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ewa_cost_min</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ewa_cost</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ewa_cost_min</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_no_improvement</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ewa_cost_min</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ewa_cost</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_no_improvement</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_no_improvement</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_no_improvement</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_no_improvement</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">&quot;Converged (lack of improvement in objective function) &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;at step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_steps</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="kc">False</span>

    <span class="nd">@_fit_context</span><span class="p">(</span><span class="n">prefer_skip_nested_validation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Learn a NMF model for the data X and returns the transformed data.</span>

<span class="sd">        This is more efficient than calling fit followed by transform.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Data matrix to be decomposed.</span>

<span class="sd">        y : Ignored</span>
<span class="sd">            Not used, present here for API consistency by convention.</span>

<span class="sd">        W : array-like of shape (n_samples, n_components), default=None</span>
<span class="sd">            If `init=&#39;custom&#39;`, it is used as initial guess for the solution.</span>
<span class="sd">            If `None`, uses the initialisation method specified in `init`.</span>

<span class="sd">        H : array-like of shape (n_components, n_features), default=None</span>
<span class="sd">            If `init=&#39;custom&#39;`, it is used as initial guess for the solution.</span>
<span class="sd">            If `None`, uses the initialisation method specified in `init`.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        W : ndarray of shape (n_samples, n_components)</span>
<span class="sd">            Transformed data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;csr&quot;</span><span class="p">,</span> <span class="s2">&quot;csc&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="k">with</span> <span class="n">config_context</span><span class="p">(</span><span class="n">assume_finite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">,</span> <span class="n">n_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="n">H</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reconstruction_err_</span> <span class="o">=</span> <span class="n">_beta_divergence</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span><span class="p">,</span> <span class="n">square_root</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_components_</span> <span class="o">=</span> <span class="n">H</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">components_</span> <span class="o">=</span> <span class="n">H</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_steps_</span> <span class="o">=</span> <span class="n">n_steps</span>

        <span class="k">return</span> <span class="n">W</span>

    <span class="k">def</span> <span class="nf">_fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">update_H</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Learn a NMF model for the data X and returns the transformed data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Data matrix to be decomposed.</span>

<span class="sd">        W : array-like of shape (n_samples, n_components), default=None</span>
<span class="sd">            If `init=&#39;custom&#39;`, it is used as initial guess for the solution.</span>
<span class="sd">            If `update_H=False`, it is initialised as an array of zeros, unless</span>
<span class="sd">            `solver=&#39;mu&#39;`, then it is filled with values calculated by</span>
<span class="sd">            `np.sqrt(X.mean() / self._n_components)`.</span>
<span class="sd">            If `None`, uses the initialisation method specified in `init`.</span>

<span class="sd">        H : array-like of shape (n_components, n_features), default=None</span>
<span class="sd">            If `init=&#39;custom&#39;`, it is used as initial guess for the solution.</span>
<span class="sd">            If `update_H=False`, it is used as a constant, to solve for W only.</span>
<span class="sd">            If `None`, uses the initialisation method specified in `init`.</span>

<span class="sd">        update_H : bool, default=True</span>
<span class="sd">            If True, both W and H will be estimated from initial guesses,</span>
<span class="sd">            this corresponds to a call to the `fit_transform` method.</span>
<span class="sd">            If False, only W will be estimated, this corresponds to a call</span>
<span class="sd">            to the `transform` method.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        W : ndarray of shape (n_samples, n_components)</span>
<span class="sd">            Transformed data.</span>

<span class="sd">        H : ndarray of shape (n_components, n_features)</span>
<span class="sd">            Factorization matrix, sometimes called &#39;dictionary&#39;.</span>

<span class="sd">        n_iter : int</span>
<span class="sd">            Actual number of started iterations over the whole dataset.</span>

<span class="sd">        n_steps : int</span>
<span class="sd">            Number of mini-batches processed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_non_negative</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s2">&quot;MiniBatchNMF (input X)&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_params</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta_loss</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;When beta_loss &lt;= 0 and X contains zeros, &quot;</span>
                <span class="s2">&quot;the solver may diverge. Please add small values &quot;</span>
                <span class="s2">&quot;to X, or use a positive beta_loss.&quot;</span>
            <span class="p">)</span>

        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># initialize or check W and H</span>
        <span class="n">W</span><span class="p">,</span> <span class="n">H</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_w_h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">update_H</span><span class="p">)</span>
        <span class="n">H_buffer</span> <span class="o">=</span> <span class="n">H</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># Initialize auxiliary matrices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_components_numerator</span> <span class="o">=</span> <span class="n">H</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_components_denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">H</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">H</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Attributes to monitor the convergence</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ewa_cost</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ewa_cost_min</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_no_improvement</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">batches</span> <span class="o">=</span> <span class="n">gen_batches</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">)</span>
        <span class="n">batches</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">cycle</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
        <span class="n">n_steps_per_iter</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">))</span>
        <span class="n">n_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">*</span> <span class="n">n_steps_per_iter</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">),</span> <span class="n">batches</span><span class="p">):</span>
            <span class="n">batch_cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_minibatch_step</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">batch</span><span class="p">],</span> <span class="n">W</span><span class="p">[</span><span class="n">batch</span><span class="p">],</span> <span class="n">H</span><span class="p">,</span> <span class="n">update_H</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">update_H</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_minibatch_convergence</span><span class="p">(</span>
                <span class="n">X</span><span class="p">[</span><span class="n">batch</span><span class="p">],</span> <span class="n">batch_cost</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">H_buffer</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">n_steps</span>
            <span class="p">):</span>
                <span class="k">break</span>

            <span class="n">H_buffer</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">H</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fresh_restarts</span><span class="p">:</span>
            <span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_solve_W</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform_max_iter</span><span class="p">)</span>

        <span class="n">n_steps</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">n_iter</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">n_steps</span> <span class="o">/</span> <span class="n">n_steps_per_iter</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">n_iter</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Maximum number of iterations </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="si">}</span><span class="s2"> reached. &quot;</span>
                    <span class="s2">&quot;Increase it to improve convergence.&quot;</span>
                <span class="p">),</span>
                <span class="n">ConvergenceWarning</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">,</span> <span class="n">n_steps</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Transform the data X according to the fitted MiniBatchNMF model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Data matrix to be transformed by the model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        W : ndarray of shape (n_samples, n_components)</span>
<span class="sd">            Transformed data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;csr&quot;</span><span class="p">,</span> <span class="s2">&quot;csc&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="n">reset</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_solve_W</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform_max_iter</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">W</span>

    <span class="nd">@_fit_context</span><span class="p">(</span><span class="n">prefer_skip_nested_validation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update the model using the data in `X` as a mini-batch.</span>

<span class="sd">        This method is expected to be called several times consecutively</span>
<span class="sd">        on different chunks of a dataset so as to implement out-of-core</span>
<span class="sd">        or online learning.</span>

<span class="sd">        This is especially useful when the whole dataset is too big to fit in</span>
<span class="sd">        memory at once (see :ref:`scaling_strategies`).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Data matrix to be decomposed.</span>

<span class="sd">        y : Ignored</span>
<span class="sd">            Not used, present here for API consistency by convention.</span>

<span class="sd">        W : array-like of shape (n_samples, n_components), default=None</span>
<span class="sd">            If `init=&#39;custom&#39;`, it is used as initial guess for the solution.</span>
<span class="sd">            Only used for the first call to `partial_fit`.</span>

<span class="sd">        H : array-like of shape (n_components, n_features), default=None</span>
<span class="sd">            If `init=&#39;custom&#39;`, it is used as initial guess for the solution.</span>
<span class="sd">            Only used for the first call to `partial_fit`.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self</span>
<span class="sd">            Returns the instance itself.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">has_components</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;components_&quot;</span><span class="p">)</span>

        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span>
            <span class="n">accept_sparse</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;csr&quot;</span><span class="p">,</span> <span class="s2">&quot;csc&quot;</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span>
            <span class="n">reset</span><span class="o">=</span><span class="ow">not</span> <span class="n">has_components</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">has_components</span><span class="p">:</span>
            <span class="c1"># This instance has not been fitted yet (fit or partial_fit)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_check_params</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">H</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_w_h</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="n">H</span><span class="p">,</span> <span class="n">update_H</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_components_numerator</span> <span class="o">=</span> <span class="n">H</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_components_denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">H</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">H</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_steps_</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">H</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">components_</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_minibatch_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">update_H</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_components_</span> <span class="o">=</span> <span class="n">H</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">components_</span> <span class="o">=</span> <span class="n">H</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_steps_</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="bp">self</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Cubert GmbH.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>